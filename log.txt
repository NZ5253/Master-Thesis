(venv) naeem@Robotik5:~/Documents/final$ ./quick_train.sh curriculum
=========================================
Parallel Parking RL Training
Mode: curriculum
=========================================

Running CURRICULUM TRAINING (6 phases)
Training until each phase reaches success threshold
This is the RECOMMENDED approach for best results!

Phases:
  1. Foundation (70% success)
  2. Random Spawn (65% success)
  3. Random Bay X (60% success)
  4. Full Bay Random (60% success)
  5. Neighbor Jitter (55% success)
  6. Random Obstacles (50% success)

================================================================================
CURRICULUM TRAINING FOR PARALLEL PARKING
================================================================================
Total phases: 7
Output directory: checkpoints/curriculum/curriculum_20260129_205818
Workers: 4
GPUs: 1
================================================================================

================================================================================
PHASE 1/7: Phase 1: Foundation - Fixed Everything
================================================================================
Phase: Phase 1: Foundation - Fixed Everything
Description: Learn basic parking with no randomization. Car always spawns at same position, bay is fixed.
Target Timesteps: 150,000,000
Success Threshold: 85.0%

Training Config:
  - Learning Rate: 3e-4
  - Entropy Coeff: 0.02
  - Batch Size: 4000
  - SGD Iterations: 10
  - Eval Interval: 5

/home/naeem/Documents/final/venv/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:483: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS="ignore::DeprecationWarning"
`UnifiedLogger` will be removed in Ray 2.7.
  return UnifiedLogger(config, logdir, loggers=None)
/home/naeem/Documents/final/venv/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS="ignore::DeprecationWarning"
The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.
  self._loggers.append(cls(self.config, self.logdir, self.trial))
/home/naeem/Documents/final/venv/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS="ignore::DeprecationWarning"
The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.
  self._loggers.append(cls(self.config, self.logdir, self.trial))
/home/naeem/Documents/final/venv/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS="ignore::DeprecationWarning"
The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.
  self._loggers.append(cls(self.config, self.logdir, self.trial))
/home/naeem/Documents/final/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:116: UserWarning: Could not parse CUBLAS_WORKSPACE_CONFIG, using default workspace size of 8519680 bytes. (Triggered internally at ../aten/src/ATen/cuda/CublasHandlePool.cpp:99.)
  return F.linear(input, self.weight, self.bias)
Created new PPO algorithm
/home/naeem/Documents/final/venv/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Could not parse CUBLAS_WORKSPACE_CONFIG, using default workspace size of 8519680 bytes. (Triggered internally at ../aten/src/ATen/cuda/CublasHandlePool.cpp:99.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

================================================================================
TRAINING PHASE 1: Phase 1: Foundation - Fixed Everything
================================================================================
Success threshold: 85.0% (must pass 5 consecutive evaluations)
Target timesteps: ~150,000,000
Max timesteps: 1,500,000,000
Training config: lr=0.0003, entropy=0.02
Starting from global timestep: 4,000
================================================================================

[Iter 5] Phase Steps: 16,000/1,500,000,000 | Reward: -354.87 | Length: 158.8 | Best Success: 0.0%

[EVAL] Iter 5 | Phase Steps: 16,000
  Success Rate: 0.0% (Threshold: 85.0%)
  Terminations: 0 success, 0 collision, 30 timeout
  Errors: pos=0.628m (min:0.628m), yaw=32.3deg
  Final State: along=0.259m, lat=0.572m, v=0.000m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=1.0
  Safety: min_clearance=0.426m
  Reward: -548.25 (mean)
  ‚úó Below threshold (need 85.0%, got 0.0%)
  üíæ Best model saved (reward: -548.25, success: 0.0%)

[Iter 10] Phase Steps: 36,000/1,500,000,000 | Reward: -418.00 | Length: 221.9 | Best Success: 0.0%

[EVAL] Iter 10 | Phase Steps: 36,000
  Success Rate: 0.0% (Threshold: 85.0%)
  Terminations: 0 success, 0 collision, 30 timeout
  Errors: pos=0.563m (min:0.563m), yaw=42.6deg
  Final State: along=0.228m, lat=0.515m, v=0.000m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=0.0
  Safety: min_clearance=0.369m
  Reward: -526.97 (mean)
  ‚úó Below threshold (need 85.0%, got 0.0%)
  üíæ Best model saved (reward: -526.97, success: 0.0%)

[Iter 15] Phase Steps: 56,000/1,500,000,000 | Reward: -379.75 | Length: 211.1 | Best Success: 0.0%

[EVAL] Iter 15 | Phase Steps: 56,000
  Success Rate: 0.0% (Threshold: 85.0%)
  Terminations: 0 success, 0 collision, 30 timeout
  Errors: pos=0.511m (min:0.511m), yaw=57.6deg
  Final State: along=0.181m, lat=0.478m, v=0.003m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=14.0
  Safety: min_clearance=0.458m
  Reward: -526.36 (mean)
  ‚úó Below threshold (need 85.0%, got 0.0%)
  üíæ Best model saved (reward: -526.36, success: 0.0%)

[Iter 20] Phase Steps: 76,000/1,500,000,000 | Reward: -423.86 | Length: 250.4 | Best Success: 0.0%

[EVAL] Iter 20 | Phase Steps: 76,000
  Success Rate: 0.0% (Threshold: 85.0%)
  Terminations: 0 success, 0 collision, 30 timeout
  Errors: pos=0.509m (min:0.509m), yaw=56.7deg
  Final State: along=0.332m, lat=0.386m, v=0.000m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=0.0
  Safety: min_clearance=0.399m
  Reward: -477.66 (mean)
  ‚úó Below threshold (need 85.0%, got 0.0%)
  üíæ Best model saved (reward: -477.66, success: 0.0%)

[Iter 25] Phase Steps: 96,000/1,500,000,000 | Reward: -430.74 | Length: 266.6 | Best Success: 0.0%

[EVAL] Iter 25 | Phase Steps: 96,000
  Success Rate: 0.0% (Threshold: 85.0%)
  Terminations: 0 success, 0 collision, 30 timeout
  Errors: pos=0.605m (min:0.605m), yaw=55.2deg
  Final State: along=0.410m, lat=0.445m, v=0.000m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=0.0
  Safety: min_clearance=0.710m
  Reward: -525.69 (mean)
  ‚úó Below threshold (need 85.0%, got 0.0%)

[Iter 30] Phase Steps: 116,000/1,500,000,000 | Reward: -463.15 | Length: 271.1 | Best Success: 0.0%

[EVAL] Iter 30 | Phase Steps: 116,000
  Success Rate: 0.0% (Threshold: 85.0%)
  Terminations: 0 success, 0 collision, 30 timeout
  Errors: pos=0.510m (min:0.510m), yaw=56.6deg
  Final State: along=0.312m, lat=0.403m, v=0.000m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=6.0
  Safety: min_clearance=0.468m
  Reward: -509.78 (mean)
  ‚úó Below threshold (need 85.0%, got 0.0%)

[Iter 35] Phase Steps: 136,000/1,500,000,000 | Reward: -483.66 | Length: 287.6 | Best Success: 0.0%

[EVAL] Iter 35 | Phase Steps: 136,000
  Success Rate: 0.0% (Threshold: 85.0%)
  Terminations: 0 success, 0 collision, 30 timeout
  Errors: pos=0.514m (min:0.514m), yaw=55.6deg
  Final State: along=0.281m, lat=0.431m, v=0.000m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=46.0
  Safety: min_clearance=0.457m
  Reward: -517.84 (mean)
  ‚úó Below threshold (need 85.0%, got 0.0%)

[Iter 40] Phase Steps: 156,000/1,500,000,000 | Reward: -402.55 | Length: 238.6 | Best Success: 0.0%

[EVAL] Iter 40 | Phase Steps: 156,000
  Success Rate: 0.0% (Threshold: 85.0%)
  Terminations: 0 success, 30 collision, 0 timeout
  Errors: pos=0.527m (min:0.527m), yaw=55.8deg
  Final State: along=0.393m, lat=0.351m, v=0.025m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=0.0
  Safety: min_clearance=0.394m
  Reward: -182.65 (mean)
  ‚úó Below threshold (need 85.0%, got 0.0%)
  üíæ Best model saved (reward: -182.65, success: 0.0%)

[Iter 45] Phase Steps: 176,000/1,500,000,000 | Reward: -326.93 | Length: 207.2 | Best Success: 0.0%

[EVAL] Iter 45 | Phase Steps: 176,000
  Success Rate: 0.0% (Threshold: 85.0%)
  Terminations: 0 success, 30 collision, 0 timeout
  Errors: pos=0.487m (min:0.487m), yaw=59.9deg
  Final State: along=0.339m, lat=0.349m, v=0.041m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=3.0
  Safety: min_clearance=0.376m
  Reward: -200.96 (mean)
  ‚úó Below threshold (need 85.0%, got 0.0%)

[Iter 50] Phase Steps: 196,000/1,500,000,000 | Reward: -368.73 | Length: 233.1 | Best Success: 0.0%

[EVAL] Iter 50 | Phase Steps: 196,000
  Success Rate: 0.0% (Threshold: 85.0%)
  Terminations: 0 success, 0 collision, 30 timeout
  Errors: pos=0.785m (min:0.785m), yaw=47.3deg
  Final State: along=0.550m, lat=0.561m, v=0.000m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=0.0
  Safety: min_clearance=0.710m
  Reward: -565.60 (mean)
  ‚úó Below threshold (need 85.0%, got 0.0%)

[Iter 55] Phase Steps: 216,000/1,500,000,000 | Reward: -460.97 | Length: 274.7 | Best Success: 0.0%

[EVAL] Iter 55 | Phase Steps: 216,000
  Success Rate: 0.0% (Threshold: 85.0%)
  Terminations: 0 success, 0 collision, 30 timeout
  Errors: pos=0.752m (min:0.752m), yaw=44.4deg
  Final State: along=0.528m, lat=0.536m, v=0.000m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=0.0
  Safety: min_clearance=0.710m
  Reward: -557.73 (mean)
  ‚úó Below threshold (need 85.0%, got 0.0%)

[Iter 60] Phase Steps: 236,000/1,500,000,000 | Reward: -405.41 | Length: 253.6 | Best Success: 0.0%

[EVAL] Iter 60 | Phase Steps: 236,000
  Success Rate: 0.0% (Threshold: 85.0%)
  Terminations: 0 success, 0 collision, 30 timeout
  Errors: pos=0.337m (min:0.337m), yaw=19.0deg
  Final State: along=0.245m, lat=0.232m, v=0.000m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=0.0
  Safety: min_clearance=0.156m
  Reward: -436.92 (mean)
  ‚úó Below threshold (need 85.0%, got 0.0%)

[Iter 65] Phase Steps: 256,000/1,500,000,000 | Reward: -277.76 | Length: 208.5 | Best Success: 0.0%

[EVAL] Iter 65 | Phase Steps: 256,000
  Success Rate: 0.0% (Threshold: 85.0%)
  Terminations: 0 success, 0 collision, 30 timeout
  Errors: pos=0.421m (min:0.421m), yaw=40.1deg
  Final State: along=0.258m, lat=0.333m, v=0.000m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=1.0
  Safety: min_clearance=0.216m
  Reward: -478.66 (mean)
  ‚úó Below threshold (need 85.0%, got 0.0%)
  ‚ö†Ô∏è  No improvement for 5 evaluations

[Iter 70] Phase Steps: 276,000/1,500,000,000 | Reward: -336.20 | Length: 231.4 | Best Success: 0.0%

[EVAL] Iter 70 | Phase Steps: 276,000
  Success Rate: 0.0% (Threshold: 85.0%)
  Terminations: 0 success, 0 collision, 30 timeout
  Errors: pos=0.437m (min:0.437m), yaw=33.0deg
  Final State: along=0.290m, lat=0.327m, v=0.000m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=0.0
  Safety: min_clearance=0.235m
  Reward: -480.96 (mean)
  ‚úó Below threshold (need 85.0%, got 0.0%)
  ‚ö†Ô∏è  No improvement for 6 evaluations

[Iter 75] Phase Steps: 296,000/1,500,000,000 | Reward: -342.84 | Length: 242.9 | Best Success: 0.0%

[EVAL] Iter 75 | Phase Steps: 296,000
  Success Rate: 0.0% (Threshold: 85.0%)
  Terminations: 0 success, 0 collision, 30 timeout
  Errors: pos=0.574m (min:0.574m), yaw=49.1deg
  Final State: along=0.405m, lat=0.407m, v=0.036m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=0.0
  Safety: min_clearance=0.327m
  Reward: -407.28 (mean)
  ‚úó Below threshold (need 85.0%, got 0.0%)
  ‚ö†Ô∏è  No improvement for 7 evaluations

[Iter 80] Phase Steps: 316,000/1,500,000,000 | Reward: -302.64 | Length: 233.1 | Best Success: 0.0%
^[
[EVAL] Iter 80 | Phase Steps: 316,000
  Success Rate: 0.0% (Threshold: 85.0%)
  Terminations: 0 success, 30 collision, 0 timeout
  Errors: pos=0.333m (min:0.333m), yaw=9.6deg
  Final State: along=0.238m, lat=0.234m, v=0.169m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=0.0
  Safety: min_clearance=0.158m
  Reward: -168.52 (mean)
  ‚úó Below threshold (need 85.0%, got 0.0%)
  üíæ Best model saved (reward: -168.52, success: 0.0%)

[Iter 85] Phase Steps: 336,000/1,500,000,000 | Reward: -292.08 | Length: 212.4 | Best Success: 0.0%

[EVAL] Iter 85 | Phase Steps: 336,000
  Success Rate: 0.0% (Threshold: 85.0%)
  Terminations: 0 success, 0 collision, 30 timeout
  Errors: pos=0.596m (min:0.596m), yaw=44.5deg
  Final State: along=0.413m, lat=0.430m, v=0.060m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=2.0
  Safety: min_clearance=0.274m
  Reward: -497.86 (mean)
  ‚úó Below threshold (need 85.0%, got 0.0%)

^[[Iter 90] Phase Steps: 356,000/1,500,000,000 | Reward: -148.83 | Length: 155.9 | Best Success: 0.0%

[EVAL] Iter 90 | Phase Steps: 356,000
  Success Rate: 0.0% (Threshold: 85.0%)
  Terminations: 0 success, 0 collision, 30 timeout
  Errors: pos=0.286m (min:0.286m), yaw=38.5deg
  Final State: along=0.135m, lat=0.253m, v=0.000m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=2.0
  Safety: min_clearance=0.196m
  Reward: -429.35 (mean)
  ‚úó Below threshold (need 85.0%, got 0.0%)

[Iter 95] Phase Steps: 376,000/1,500,000,000 | Reward: 19.77 | Length: 119.5 | Best Success: 0.0%

[EVAL] Iter 95 | Phase Steps: 376,000
  Success Rate: 0.0% (Threshold: 85.0%)
  Terminations: 0 success, 0 collision, 30 timeout
  Errors: pos=0.290m (min:0.290m), yaw=30.0deg
  Final State: along=0.127m, lat=0.261m, v=0.000m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=9.0
  Safety: min_clearance=0.231m
  Reward: -428.26 (mean)
  ‚úó Below threshold (need 85.0%, got 0.0%)

[Iter 100] Phase Steps: 396,000/1,500,000,000 | Reward: 53.02 | Length: 111.0 | Best Success: 0.0%

[EVAL] Iter 100 | Phase Steps: 396,000
  Success Rate: 0.0% (Threshold: 85.0%)
  Terminations: 0 success, 0 collision, 30 timeout
  Errors: pos=0.222m (min:0.222m), yaw=15.0deg
  Final State: along=0.012m, lat=0.222m, v=0.000m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=11.0
  Safety: min_clearance=0.223m
  Reward: -413.31 (mean)
  ‚úó Below threshold (need 85.0%, got 0.0%)

[Iter 105] Phase Steps: 416,000/1,500,000,000 | Reward: 86.42 | Length: 105.9 | Best Success: 0.0%

[EVAL] Iter 105 | Phase Steps: 416,000
  Success Rate: 0.0% (Threshold: 85.0%)
  Terminations: 0 success, 0 collision, 30 timeout
  Errors: pos=0.296m (min:0.296m), yaw=7.2deg
  Final State: along=0.049m, lat=0.292m, v=0.000m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=9.0
  Safety: min_clearance=0.251m
  Reward: -432.24 (mean)
  ‚úó Below threshold (need 85.0%, got 0.0%)
  ‚ö†Ô∏è  No improvement for 5 evaluations

[Iter 110] Phase Steps: 436,000/1,500,000,000 | Reward: 36.86 | Length: 116.2 | Best Success: 0.0%

[EVAL] Iter 110 | Phase Steps: 436,000
  Success Rate: 0.0% (Threshold: 85.0%)
  Terminations: 0 success, 0 collision, 30 timeout
  Errors: pos=0.279m (min:0.279m), yaw=5.9deg
  Final State: along=0.069m, lat=0.271m, v=0.000m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=9.0
  Safety: min_clearance=0.237m
  Reward: -426.35 (mean)
  ‚úó Below threshold (need 85.0%, got 0.0%)
  ‚ö†Ô∏è  No improvement for 6 evaluations

[Iter 115] Phase Steps: 456,000/1,500,000,000 | Reward: 134.51 | Length: 69.5 | Best Success: 0.0%

[EVAL] Iter 115 | Phase Steps: 456,000
  Success Rate: 100.0% (Threshold: 85.0%)
  Terminations: 30 success, 0 collision, 0 timeout
  Errors: pos=0.210m (min:0.210m), yaw=3.3deg
  Final State: along=0.068m, lat=0.199m, v=0.082m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=9.0
  Safety: min_clearance=0.216m
  Reward: 242.31 (mean)
  ‚úì PASSED threshold! (1/5 consecutive passes needed)
  üíæ Best model saved (reward: 242.31, success: 100.0%)

[Iter 120] Phase Steps: 476,000/1,500,000,000 | Reward: 169.71 | Length: 57.9 | Best Success: 100.0%

[EVAL] Iter 120 | Phase Steps: 476,000
  Success Rate: 100.0% (Threshold: 85.0%)
  Terminations: 30 success, 0 collision, 0 timeout
  Errors: pos=0.216m (min:0.216m), yaw=2.4deg
  Final State: along=0.092m, lat=0.196m, v=0.123m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=11.0
  Safety: min_clearance=0.211m
  Reward: 243.34 (mean)
  ‚úì PASSED threshold! (2/5 consecutive passes needed)
  üíæ Best model saved (reward: 243.34, success: 100.0%)

[Iter 125] Phase Steps: 496,000/1,500,000,000 | Reward: 191.42 | Length: 48.6 | Best Success: 100.0%

[EVAL] Iter 125 | Phase Steps: 496,000
  Success Rate: 0.0% (Threshold: 85.0%)
  Terminations: 0 success, 0 collision, 30 timeout
  Errors: pos=0.248m (min:0.248m), yaw=7.2deg
  Final State: along=0.067m, lat=0.239m, v=0.024m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=98.0
  Safety: min_clearance=0.252m
  Reward: -416.90 (mean)
  ‚úó Below threshold (need 85.0%, got 0.0%)

[Iter 130] Phase Steps: 516,000/1,500,000,000 | Reward: 228.46 | Length: 34.4 | Best Success: 100.0%

[EVAL] Iter 130 | Phase Steps: 516,000
  Success Rate: 100.0% (Threshold: 85.0%)
  Terminations: 30 success, 0 collision, 0 timeout
  Errors: pos=0.203m (min:0.203m), yaw=0.9deg
  Final State: along=0.040m, lat=0.199m, v=0.180m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=11.0
  Safety: min_clearance=0.244m
  Reward: 244.47 (mean)
  ‚úì PASSED threshold! (1/5 consecutive passes needed)
  üíæ Best model saved (reward: 244.47, success: 100.0%)

[Iter 135] Phase Steps: 536,000/1,500,000,000 | Reward: 247.95 | Length: 32.0 | Best Success: 100.0%

[EVAL] Iter 135 | Phase Steps: 536,000
  Success Rate: 100.0% (Threshold: 85.0%)
  Terminations: 30 success, 0 collision, 0 timeout
  Errors: pos=0.180m (min:0.180m), yaw=2.8deg
  Final State: along=0.031m, lat=0.178m, v=0.186m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=9.0
  Safety: min_clearance=0.238m
  Reward: 246.05 (mean)
  ‚úì PASSED threshold! (2/5 consecutive passes needed)
  üíæ Best model saved (reward: 246.05, success: 100.0%)

[Iter 140] Phase Steps: 556,000/1,500,000,000 | Reward: 233.06 | Length: 31.4 | Best Success: 100.0%

[EVAL] Iter 140 | Phase Steps: 556,000
  Success Rate: 100.0% (Threshold: 85.0%)
  Terminations: 30 success, 0 collision, 0 timeout
  Errors: pos=0.168m (min:0.168m), yaw=2.5deg
  Final State: along=0.025m, lat=0.166m, v=0.169m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=11.0
  Safety: min_clearance=0.248m
  Reward: 246.91 (mean)
  ‚úì PASSED threshold! (3/5 consecutive passes needed)
  üíæ Best model saved (reward: 246.91, success: 100.0%)

[Iter 145] Phase Steps: 576,000/1,500,000,000 | Reward: 239.24 | Length: 29.9 | Best Success: 100.0%

[EVAL] Iter 145 | Phase Steps: 576,000
  Success Rate: 100.0% (Threshold: 85.0%)
  Terminations: 30 success, 0 collision, 0 timeout
  Errors: pos=0.191m (min:0.191m), yaw=1.5deg
  Final State: along=0.038m, lat=0.187m, v=0.179m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=11.0
  Safety: min_clearance=0.248m
  Reward: 246.25 (mean)
  ‚úì PASSED threshold! (4/5 consecutive passes needed)

[Iter 150] Phase Steps: 596,000/1,500,000,000 | Reward: 247.25 | Length: 32.1 | Best Success: 100.0%

[EVAL] Iter 150 | Phase Steps: 596,000
  Success Rate: 100.0% (Threshold: 85.0%)
  Terminations: 30 success, 0 collision, 0 timeout
  Errors: pos=0.192m (min:0.192m), yaw=2.8deg
  Final State: along=0.035m, lat=0.189m, v=0.183m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=8.0
  Safety: min_clearance=0.268m
  Reward: 245.66 (mean)
  ‚úì PASSED threshold! (5/5 consecutive passes needed)

================================================================================
‚úì‚úì‚úì PHASE 1 COMPLETE - SUCCESS THRESHOLD ACHIEVED!
================================================================================
Phase timesteps: 596,000
Success rate: 100.0% (threshold: 85.0%)
Best success rate: 100.0%
Best eval reward: 246.91
================================================================================


================================================================================
PHASE 1 SUMMARY
================================================================================
  Phase timesteps: 596,000
  Best eval reward: 246.91
  Best success rate: 100.0%
  Status: ‚úì SUCCESS (threshold 85.0% achieved)
================================================================================

================================================================================
PHASE 2/7: Phase 2: Random Spawn Position
================================================================================
Phase: Phase 2: Random Spawn Position
Description: Bay is fixed, but spawn position varies (x, y, yaw). Learn to approach from different angles.
Target Timesteps: 40,000,000
Success Threshold: 80.0%

Training Config:
  - Learning Rate: 3e-4
  - Entropy Coeff: 0.015
  - Batch Size: 4000
  - SGD Iterations: 10
  - Eval Interval: 10

/home/naeem/Documents/final/venv/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:483: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS="ignore::DeprecationWarning"
`UnifiedLogger` will be removed in Ray 2.7.
  return UnifiedLogger(config, logdir, loggers=None)
/home/naeem/Documents/final/venv/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS="ignore::DeprecationWarning"
The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.
  self._loggers.append(cls(self.config, self.logdir, self.trial))
/home/naeem/Documents/final/venv/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS="ignore::DeprecationWarning"
The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.
  self._loggers.append(cls(self.config, self.logdir, self.trial))
/home/naeem/Documents/final/venv/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS="ignore::DeprecationWarning"
The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.
  self._loggers.append(cls(self.config, self.logdir, self.trial))
Loading policy weights from: checkpoints/curriculum/curriculum_20260129_205818/phase1_foundation/final_checkpoint
‚úì Loaded 13 weight tensors from checkpoint
Policy weights loaded successfully (warm start)

================================================================================
TRAINING PHASE 2: Phase 2: Random Spawn Position
================================================================================
Success threshold: 80.0% (must pass 5 consecutive evaluations)
Target timesteps: ~40,000,000
Max timesteps: 400,000,000
Training config: lr=0.0003, entropy=0.015
Starting from global timestep: 4,000
================================================================================

[Iter 5] Phase Steps: 16,000/400,000,000 | Reward: -1006.99 | Length: 143.4 | Best Success: 0.0%
[Iter 10] Phase Steps: 36,000/400,000,000 | Reward: -843.83 | Length: 213.4 | Best Success: 0.0%

[EVAL] Iter 10 | Phase Steps: 36,000
  Success Rate: 0.0% (Threshold: 80.0%)
  Terminations: 0 success, 0 collision, 30 timeout
  Errors: pos=1.045m (min:0.526m), yaw=14.1deg
  Final State: along=0.791m, lat=0.662m, v=0.006m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=0.1
  Safety: min_clearance=0.256m
  Reward: -389.77 (mean)
  ‚úó Below threshold (need 80.0%, got 0.0%)
  üíæ Best model saved (reward: -389.77, success: 0.0%)

[Iter 15] Phase Steps: 56,000/400,000,000 | Reward: -585.06 | Length: 246.1 | Best Success: 0.0%
[Iter 20] Phase Steps: 76,000/400,000,000 | Reward: -507.24 | Length: 244.4 | Best Success: 0.0%

[EVAL] Iter 20 | Phase Steps: 76,000
  Success Rate: 0.0% (Threshold: 80.0%)
  Terminations: 0 success, 0 collision, 30 timeout
  Errors: pos=0.482m (min:0.207m), yaw=8.1deg
  Final State: along=0.276m, lat=0.390m, v=0.020m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=3.4
  Safety: min_clearance=0.281m
  Reward: -263.41 (mean)
  ‚úó Below threshold (need 80.0%, got 0.0%)
  üíæ Best model saved (reward: -263.41, success: 0.0%)

[Iter 25] Phase Steps: 96,000/400,000,000 | Reward: -420.98 | Length: 265.6 | Best Success: 0.0%
[Iter 30] Phase Steps: 116,000/400,000,000 | Reward: -441.63 | Length: 249.8 | Best Success: 0.0%

[EVAL] Iter 30 | Phase Steps: 116,000
  Success Rate: 3.3% (Threshold: 80.0%)
  Terminations: 1 success, 0 collision, 29 timeout
  Errors: pos=0.275m (min:0.154m), yaw=7.4deg
  Final State: along=0.054m, lat=0.268m, v=0.078m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=4.9
  Safety: min_clearance=0.201m
  Reward: -225.69 (mean)
  ‚úó Below threshold (need 80.0%, got 3.3%)
  üíæ Best model saved (reward: -225.69, success: 3.3%)

[Iter 35] Phase Steps: 136,000/400,000,000 | Reward: -212.58 | Length: 238.0 | Best Success: 3.3%
[Iter 40] Phase Steps: 156,000/400,000,000 | Reward: 179.07 | Length: 218.4 | Best Success: 3.3%

[EVAL] Iter 40 | Phase Steps: 156,000
  Success Rate: 13.3% (Threshold: 80.0%)
  Terminations: 4 success, 3 collision, 23 timeout
  Errors: pos=0.237m (min:0.147m), yaw=8.4deg
  Final State: along=0.057m, lat=0.225m, v=0.038m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=10.9
  Safety: min_clearance=0.181m
  Reward: -49.05 (mean)
  ‚úó Below threshold (need 80.0%, got 13.3%)
  üíæ Best model saved (reward: -49.05, success: 13.3%)

[Iter 45] Phase Steps: 176,000/400,000,000 | Reward: 425.24 | Length: 178.5 | Best Success: 13.3%
[Iter 50] Phase Steps: 196,000/400,000,000 | Reward: 597.57 | Length: 167.8 | Best Success: 13.3%

[EVAL] Iter 50 | Phase Steps: 196,000
  Success Rate: 16.7% (Threshold: 80.0%)
  Terminations: 5 success, 7 collision, 18 timeout
  Errors: pos=0.216m (min:0.141m), yaw=15.0deg
  Final State: along=0.079m, lat=0.192m, v=0.061m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=10.5
  Safety: min_clearance=0.139m
  Reward: 23.19 (mean)
  ‚úó Below threshold (need 80.0%, got 16.7%)
  üíæ Best model saved (reward: 23.19, success: 16.7%)

[Iter 55] Phase Steps: 216,000/400,000,000 | Reward: 267.01 | Length: 142.6 | Best Success: 16.7%
[Iter 60] Phase Steps: 236,000/400,000,000 | Reward: 613.32 | Length: 148.9 | Best Success: 16.7%

[EVAL] Iter 60 | Phase Steps: 236,000
  Success Rate: 43.3% (Threshold: 80.0%)
  Terminations: 13 success, 5 collision, 12 timeout
  Errors: pos=0.171m (min:0.114m), yaw=8.7deg
  Final State: along=0.055m, lat=0.154m, v=0.083m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=9.2
  Safety: min_clearance=0.165m
  Reward: 560.14 (mean)
  ‚úó Below threshold (need 80.0%, got 43.3%)
  üíæ Best model saved (reward: 560.14, success: 43.3%)

[Iter 65] Phase Steps: 256,000/400,000,000 | Reward: 498.11 | Length: 152.5 | Best Success: 43.3%
[Iter 70] Phase Steps: 276,000/400,000,000 | Reward: 755.79 | Length: 111.1 | Best Success: 43.3%

[EVAL] Iter 70 | Phase Steps: 276,000
  Success Rate: 56.7% (Threshold: 80.0%)
  Terminations: 17 success, 5 collision, 8 timeout
  Errors: pos=0.170m (min:0.114m), yaw=7.7deg
  Final State: along=0.057m, lat=0.154m, v=0.091m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=7.1
  Safety: min_clearance=0.170m
  Reward: 429.36 (mean)
  ‚úó Below threshold (need 80.0%, got 56.7%)
  üíæ Best model saved (reward: 429.36, success: 56.7%)

[Iter 75] Phase Steps: 296,000/400,000,000 | Reward: 842.03 | Length: 76.2 | Best Success: 56.7%
[Iter 80] Phase Steps: 316,000/400,000,000 | Reward: 866.15 | Length: 95.5 | Best Success: 56.7%

[EVAL] Iter 80 | Phase Steps: 316,000
  Success Rate: 56.7% (Threshold: 80.0%)
  Terminations: 17 success, 5 collision, 8 timeout
  Errors: pos=0.171m (min:0.113m), yaw=9.6deg
  Final State: along=0.055m, lat=0.155m, v=0.091m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=8.3
  Safety: min_clearance=0.163m
  Reward: 290.13 (mean)
  ‚úó Below threshold (need 80.0%, got 56.7%)

[Iter 85] Phase Steps: 336,000/400,000,000 | Reward: 890.85 | Length: 72.7 | Best Success: 56.7%
[Iter 90] Phase Steps: 356,000/400,000,000 | Reward: 996.63 | Length: 55.1 | Best Success: 56.7%

[EVAL] Iter 90 | Phase Steps: 356,000
  Success Rate: 93.3% (Threshold: 80.0%)
  Terminations: 28 success, 0 collision, 2 timeout
  Errors: pos=0.144m (min:0.101m), yaw=4.8deg
  Final State: along=0.038m, lat=0.136m, v=0.108m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=7.9
  Safety: min_clearance=0.202m
  Reward: 1167.12 (mean)
  ‚úì PASSED threshold! (1/5 consecutive passes needed)
  üíæ Best model saved (reward: 1167.12, success: 93.3%)

[Iter 95] Phase Steps: 376,000/400,000,000 | Reward: 1081.39 | Length: 53.3 | Best Success: 93.3%
[Iter 100] Phase Steps: 396,000/400,000,000 | Reward: 1053.15 | Length: 48.8 | Best Success: 93.3%

[EVAL] Iter 100 | Phase Steps: 396,000
  Success Rate: 96.7% (Threshold: 80.0%)
  Terminations: 29 success, 1 collision, 0 timeout
  Errors: pos=0.139m (min:0.103m), yaw=5.3deg
  Final State: along=0.033m, lat=0.132m, v=0.118m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=7.8
  Safety: min_clearance=0.194m
  Reward: 966.19 (mean)
  ‚úì PASSED threshold! (2/5 consecutive passes needed)
  üíæ Best model saved (reward: 966.19, success: 96.7%)

[Iter 105] Phase Steps: 416,000/400,000,000 | Reward: 1073.75 | Length: 44.4 | Best Success: 96.7%
[Iter 110] Phase Steps: 436,000/400,000,000 | Reward: 1014.70 | Length: 43.5 | Best Success: 96.7%

[EVAL] Iter 110 | Phase Steps: 436,000
  Success Rate: 100.0% (Threshold: 80.0%)
  Terminations: 30 success, 0 collision, 0 timeout
  Errors: pos=0.131m (min:0.101m), yaw=4.7deg
  Final State: along=0.029m, lat=0.125m, v=0.113m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=7.2
  Safety: min_clearance=0.210m
  Reward: 1176.87 (mean)
  ‚úì PASSED threshold! (3/5 consecutive passes needed)
  üíæ Best model saved (reward: 1176.87, success: 100.0%)

[Iter 115] Phase Steps: 456,000/400,000,000 | Reward: 1067.19 | Length: 49.0 | Best Success: 100.0%
[Iter 120] Phase Steps: 476,000/400,000,000 | Reward: 907.88 | Length: 44.5 | Best Success: 100.0%

[EVAL] Iter 120 | Phase Steps: 476,000
  Success Rate: 96.7% (Threshold: 80.0%)
  Terminations: 29 success, 0 collision, 1 timeout
  Errors: pos=0.146m (min:0.101m), yaw=4.4deg
  Final State: along=0.032m, lat=0.140m, v=0.110m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=7.4
  Safety: min_clearance=0.207m
  Reward: 1177.33 (mean)
  ‚úì PASSED threshold! (4/5 consecutive passes needed)
  üíæ Best model saved (reward: 1177.33, success: 96.7%)

[Iter 125] Phase Steps: 496,000/400,000,000 | Reward: 1093.91 | Length: 37.8 | Best Success: 100.0%
[Iter 130] Phase Steps: 516,000/400,000,000 | Reward: 1040.24 | Length: 40.7 | Best Success: 100.0%

[EVAL] Iter 130 | Phase Steps: 516,000
  Success Rate: 96.7% (Threshold: 80.0%)
  Terminations: 29 success, 0 collision, 1 timeout
  Errors: pos=0.143m (min:0.107m), yaw=5.9deg
  Final State: along=0.024m, lat=0.139m, v=0.111m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=6.2
  Safety: min_clearance=0.218m
  Reward: 1191.87 (mean)
  ‚úì PASSED threshold! (5/5 consecutive passes needed)
  üíæ Best model saved (reward: 1191.87, success: 96.7%)

================================================================================
‚úì‚úì‚úì PHASE 2 COMPLETE - SUCCESS THRESHOLD ACHIEVED!
================================================================================
Phase timesteps: 516,000
Success rate: 96.7% (threshold: 80.0%)
Best success rate: 100.0%
Best eval reward: 1191.87
================================================================================


================================================================================
PHASE 2 SUMMARY
================================================================================
  Phase timesteps: 516,000
  Best eval reward: 1191.87
  Best success rate: 100.0%
  Status: ‚úì SUCCESS (threshold 80.0% achieved)
================================================================================

================================================================================
PHASE 3/7: Phase 3: Random Bay X Position
================================================================================
Phase: Phase 3: Random Bay X Position
Description: Bay can shift left/right. Spawn still varies. Goal calculation must adapt.
Target Timesteps: 50,000,000
Success Threshold: 75.0%

Training Config:
  - Learning Rate: 3e-4
  - Entropy Coeff: 0.01
  - Batch Size: 4000
  - SGD Iterations: 10
  - Eval Interval: 10

Loading policy weights from: checkpoints/curriculum/curriculum_20260129_205818/phase2_random_spawn/final_checkpoint
‚úì Loaded 13 weight tensors from checkpoint
Policy weights loaded successfully (warm start)

================================================================================
TRAINING PHASE 3: Phase 3: Random Bay X Position
================================================================================
Success threshold: 75.0% (must pass 5 consecutive evaluations)
Target timesteps: ~50,000,000
Max timesteps: 500,000,000
Training config: lr=0.0003, entropy=0.01
Starting from global timestep: 4,000
================================================================================

[Iter 5] Phase Steps: 16,000/500,000,000 | Reward: -555.64 | Length: 161.6 | Best Success: 0.0%
[Iter 10] Phase Steps: 36,000/500,000,000 | Reward: 12.89 | Length: 162.8 | Best Success: 0.0%

[EVAL] Iter 10 | Phase Steps: 36,000
  Success Rate: 66.7% (Threshold: 75.0%)
  Terminations: 20 success, 6 collision, 4 timeout
  Errors: pos=0.236m (min:0.031m), yaw=20.7deg
  Final State: along=0.141m, lat=0.175m, v=0.203m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=4.8
  Safety: min_clearance=0.146m
  Reward: 543.64 (mean)
  ‚úó Below threshold (need 75.0%, got 66.7%)
  üíæ Best model saved (reward: 543.64, success: 66.7%)

[Iter 15] Phase Steps: 56,000/500,000,000 | Reward: 105.55 | Length: 106.2 | Best Success: 66.7%
[Iter 20] Phase Steps: 76,000/500,000,000 | Reward: 523.12 | Length: 94.3 | Best Success: 66.7%

[EVAL] Iter 20 | Phase Steps: 76,000
  Success Rate: 66.7% (Threshold: 75.0%)
  Terminations: 20 success, 7 collision, 3 timeout
  Errors: pos=0.209m (min:0.040m), yaw=16.1deg
  Final State: along=0.133m, lat=0.147m, v=0.171m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=5.6
  Safety: min_clearance=0.187m
  Reward: 555.46 (mean)
  ‚úó Below threshold (need 75.0%, got 66.7%)
  üíæ Best model saved (reward: 555.46, success: 66.7%)

[Iter 25] Phase Steps: 96,000/500,000,000 | Reward: 601.02 | Length: 71.8 | Best Success: 66.7%
[Iter 30] Phase Steps: 116,000/500,000,000 | Reward: 665.23 | Length: 86.8 | Best Success: 66.7%

[EVAL] Iter 30 | Phase Steps: 116,000
  Success Rate: 90.0% (Threshold: 75.0%)
  Terminations: 27 success, 1 collision, 2 timeout
  Errors: pos=0.142m (min:0.046m), yaw=7.3deg
  Final State: along=0.075m, lat=0.115m, v=0.128m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=6.0
  Safety: min_clearance=0.204m
  Reward: 955.35 (mean)
  ‚úì PASSED threshold! (1/5 consecutive passes needed)
  üíæ Best model saved (reward: 955.35, success: 90.0%)

[Iter 35] Phase Steps: 136,000/500,000,000 | Reward: 925.11 | Length: 62.5 | Best Success: 90.0%
[Iter 40] Phase Steps: 156,000/500,000,000 | Reward: 967.36 | Length: 40.6 | Best Success: 90.0%

[EVAL] Iter 40 | Phase Steps: 156,000
  Success Rate: 93.3% (Threshold: 75.0%)
  Terminations: 28 success, 1 collision, 1 timeout
  Errors: pos=0.121m (min:0.038m), yaw=6.9deg
  Final State: along=0.071m, lat=0.095m, v=0.119m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=6.3
  Safety: min_clearance=0.199m
  Reward: 1187.92 (mean)
  ‚úì PASSED threshold! (2/5 consecutive passes needed)
  üíæ Best model saved (reward: 1187.92, success: 93.3%)

[Iter 45] Phase Steps: 176,000/500,000,000 | Reward: 925.80 | Length: 41.7 | Best Success: 93.3%
[Iter 50] Phase Steps: 196,000/500,000,000 | Reward: 947.12 | Length: 36.1 | Best Success: 93.3%

[EVAL] Iter 50 | Phase Steps: 196,000
  Success Rate: 93.3% (Threshold: 75.0%)
  Terminations: 28 success, 0 collision, 2 timeout
  Errors: pos=0.139m (min:0.076m), yaw=7.2deg
  Final State: along=0.078m, lat=0.111m, v=0.098m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=6.2
  Safety: min_clearance=0.230m
  Reward: 1033.78 (mean)
  ‚úì PASSED threshold! (3/5 consecutive passes needed)

[Iter 55] Phase Steps: 216,000/500,000,000 | Reward: 1023.99 | Length: 38.0 | Best Success: 93.3%
[Iter 60] Phase Steps: 236,000/500,000,000 | Reward: 1136.64 | Length: 42.6 | Best Success: 93.3%

[EVAL] Iter 60 | Phase Steps: 236,000
  Success Rate: 93.3% (Threshold: 75.0%)
  Terminations: 28 success, 0 collision, 2 timeout
  Errors: pos=0.132m (min:0.048m), yaw=7.7deg
  Final State: along=0.068m, lat=0.107m, v=0.095m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=6.0
  Safety: min_clearance=0.203m
  Reward: 1029.56 (mean)
  ‚úì PASSED threshold! (4/5 consecutive passes needed)

[Iter 65] Phase Steps: 256,000/500,000,000 | Reward: 1114.09 | Length: 34.1 | Best Success: 93.3%
[Iter 70] Phase Steps: 276,000/500,000,000 | Reward: 987.27 | Length: 40.1 | Best Success: 93.3%

[EVAL] Iter 70 | Phase Steps: 276,000
  Success Rate: 93.3% (Threshold: 75.0%)
  Terminations: 28 success, 0 collision, 2 timeout
  Errors: pos=0.133m (min:0.057m), yaw=7.9deg
  Final State: along=0.069m, lat=0.107m, v=0.095m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=6.2
  Safety: min_clearance=0.221m
  Reward: 1184.54 (mean)
  ‚úì PASSED threshold! (5/5 consecutive passes needed)

================================================================================
‚úì‚úì‚úì PHASE 3 COMPLETE - SUCCESS THRESHOLD ACHIEVED!
================================================================================
Phase timesteps: 276,000
Success rate: 93.3% (threshold: 75.0%)
Best success rate: 93.3%
Best eval reward: 1187.92
================================================================================


================================================================================
PHASE 3 SUMMARY
================================================================================
  Phase timesteps: 276,000
  Best eval reward: 1187.92
  Best success rate: 93.3%
  Status: ‚úì SUCCESS (threshold 75.0% achieved)
================================================================================

================================================================================
PHASE 4/7: Phase 4a: Small Bay Y Variation
================================================================================
Phase: Phase 4a: Small Bay Y Variation
Description: Bay Y varies ¬±10cm. Bridge between fixed-Y and full randomization.
Target Timesteps: 200,000,000
Success Threshold: 70.0%

Training Config:
  - Learning Rate: 3e-4
  - Entropy Coeff: 0.015
  - Batch Size: 6000
  - SGD Iterations: 12
  - Eval Interval: 10

Loading policy weights from: checkpoints/curriculum/curriculum_20260129_205818/phase3_random_bay_x/final_checkpoint
‚úì Loaded 13 weight tensors from checkpoint
Policy weights loaded successfully (warm start)

================================================================================
TRAINING PHASE 4: Phase 4a: Small Bay Y Variation
================================================================================
Success threshold: 70.0% (must pass 5 consecutive evaluations)
Target timesteps: ~200,000,000
Max timesteps: 2,000,000,000
Training config: lr=0.0003, entropy=0.015
Starting from global timestep: 6,400
================================================================================

[Iter 5] Phase Steps: 25,600/2,000,000,000 | Reward: -946.11 | Length: 160.4 | Best Success: 0.0%
[Iter 10] Phase Steps: 57,600/2,000,000,000 | Reward: -717.00 | Length: 222.6 | Best Success: 0.0%

[EVAL] Iter 10 | Phase Steps: 57,600
  Success Rate: 0.0% (Threshold: 70.0%)
  Terminations: 0 success, 1 collision, 29 timeout
  Errors: pos=0.923m (min:0.228m), yaw=17.4deg
  Final State: along=0.596m, lat=0.679m, v=0.045m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=5.0
  Safety: min_clearance=0.213m
  Reward: -499.04 (mean)
  ‚úó Below threshold (need 70.0%, got 0.0%)
  üíæ Best model saved (reward: -499.04, success: 0.0%)

[Iter 15] Phase Steps: 89,600/2,000,000,000 | Reward: -620.47 | Length: 244.9 | Best Success: 0.0%
[Iter 20] Phase Steps: 121,600/2,000,000,000 | Reward: -576.22 | Length: 258.5 | Best Success: 0.0%

[EVAL] Iter 20 | Phase Steps: 121,600
  Success Rate: 0.0% (Threshold: 70.0%)
  Terminations: 0 success, 0 collision, 30 timeout
  Errors: pos=0.791m (min:0.411m), yaw=21.2deg
  Final State: along=0.394m, lat=0.672m, v=0.036m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=8.0
  Safety: min_clearance=0.214m
  Reward: -411.24 (mean)
  ‚úó Below threshold (need 70.0%, got 0.0%)
  üíæ Best model saved (reward: -411.24, success: 0.0%)

[Iter 25] Phase Steps: 153,600/2,000,000,000 | Reward: -543.91 | Length: 239.9 | Best Success: 0.0%
[Iter 30] Phase Steps: 185,600/2,000,000,000 | Reward: -493.53 | Length: 239.5 | Best Success: 0.0%

[EVAL] Iter 30 | Phase Steps: 185,600
  Success Rate: 0.0% (Threshold: 70.0%)
  Terminations: 0 success, 1 collision, 29 timeout
  Errors: pos=0.849m (min:0.345m), yaw=17.2deg
  Final State: along=0.614m, lat=0.542m, v=0.050m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=11.1
  Safety: min_clearance=0.205m
  Reward: -472.94 (mean)
  ‚úó Below threshold (need 70.0%, got 0.0%)

[Iter 35] Phase Steps: 217,600/2,000,000,000 | Reward: -384.03 | Length: 269.4 | Best Success: 0.0%
[Iter 40] Phase Steps: 249,600/2,000,000,000 | Reward: -478.41 | Length: 225.2 | Best Success: 0.0%

[EVAL] Iter 40 | Phase Steps: 249,600
  Success Rate: 0.0% (Threshold: 70.0%)
  Terminations: 0 success, 0 collision, 30 timeout
  Errors: pos=0.501m (min:0.247m), yaw=31.0deg
  Final State: along=0.247m, lat=0.424m, v=0.006m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=9.0
  Safety: min_clearance=0.132m
  Reward: -363.69 (mean)
  ‚úó Below threshold (need 70.0%, got 0.0%)
  üíæ Best model saved (reward: -363.69, success: 0.0%)

[Iter 45] Phase Steps: 281,600/2,000,000,000 | Reward: -417.24 | Length: 249.7 | Best Success: 0.0%
[Iter 50] Phase Steps: 313,600/2,000,000,000 | Reward: -473.30 | Length: 235.4 | Best Success: 0.0%

[EVAL] Iter 50 | Phase Steps: 313,600
  Success Rate: 0.0% (Threshold: 70.0%)
  Terminations: 0 success, 3 collision, 27 timeout
  Errors: pos=0.506m (min:0.182m), yaw=44.9deg
  Final State: along=0.331m, lat=0.344m, v=0.038m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=5.4
  Safety: min_clearance=0.214m
  Reward: -355.44 (mean)
  ‚úó Below threshold (need 70.0%, got 0.0%)
  üíæ Best model saved (reward: -355.44, success: 0.0%)

[Iter 55] Phase Steps: 345,600/2,000,000,000 | Reward: -414.89 | Length: 245.8 | Best Success: 0.0%
[Iter 60] Phase Steps: 377,600/2,000,000,000 | Reward: -319.92 | Length: 234.5 | Best Success: 0.0%

[EVAL] Iter 60 | Phase Steps: 377,600
  Success Rate: 6.7% (Threshold: 70.0%)
  Terminations: 2 success, 1 collision, 27 timeout
  Errors: pos=0.356m (min:0.099m), yaw=30.5deg
  Final State: along=0.204m, lat=0.266m, v=0.014m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=5.5
  Safety: min_clearance=0.206m
  Reward: -70.07 (mean)
  ‚úó Below threshold (need 70.0%, got 6.7%)
  üíæ Best model saved (reward: -70.07, success: 6.7%)

[Iter 65] Phase Steps: 409,600/2,000,000,000 | Reward: -216.05 | Length: 236.9 | Best Success: 6.7%
[Iter 70] Phase Steps: 441,600/2,000,000,000 | Reward: -111.28 | Length: 227.5 | Best Success: 6.7%

[EVAL] Iter 70 | Phase Steps: 441,600
  Success Rate: 46.7% (Threshold: 70.0%)
  Terminations: 14 success, 0 collision, 16 timeout
  Errors: pos=0.287m (min:0.026m), yaw=23.6deg
  Final State: along=0.146m, lat=0.228m, v=0.055m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=6.3
  Safety: min_clearance=0.205m
  Reward: 735.45 (mean)
  ‚úó Below threshold (need 70.0%, got 46.7%)
  üíæ Best model saved (reward: 735.45, success: 46.7%)

[Iter 75] Phase Steps: 473,600/2,000,000,000 | Reward: 3.57 | Length: 200.8 | Best Success: 46.7%
[Iter 80] Phase Steps: 505,600/2,000,000,000 | Reward: 51.04 | Length: 199.0 | Best Success: 46.7%

[EVAL] Iter 80 | Phase Steps: 505,600
  Success Rate: 43.3% (Threshold: 70.0%)
  Terminations: 13 success, 3 collision, 14 timeout
  Errors: pos=0.260m (min:0.054m), yaw=17.2deg
  Final State: along=0.128m, lat=0.207m, v=0.077m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=9.3
  Safety: min_clearance=0.173m
  Reward: 456.54 (mean)
  ‚úó Below threshold (need 70.0%, got 43.3%)

[Iter 85] Phase Steps: 537,600/2,000,000,000 | Reward: 217.44 | Length: 165.2 | Best Success: 46.7%
[Iter 90] Phase Steps: 569,600/2,000,000,000 | Reward: 553.84 | Length: 125.5 | Best Success: 46.7%

[EVAL] Iter 90 | Phase Steps: 569,600
  Success Rate: 36.7% (Threshold: 70.0%)
  Terminations: 11 success, 7 collision, 12 timeout
  Errors: pos=0.266m (min:0.059m), yaw=19.5deg
  Final State: along=0.140m, lat=0.205m, v=0.102m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=6.4
  Safety: min_clearance=0.134m
  Reward: 227.93 (mean)
  ‚úó Below threshold (need 70.0%, got 36.7%)

[Iter 95] Phase Steps: 601,600/2,000,000,000 | Reward: 644.60 | Length: 128.5 | Best Success: 46.7%
[Iter 100] Phase Steps: 633,600/2,000,000,000 | Reward: 693.86 | Length: 134.5 | Best Success: 46.7%

[EVAL] Iter 100 | Phase Steps: 633,600
  Success Rate: 56.7% (Threshold: 70.0%)
  Terminations: 17 success, 3 collision, 10 timeout
  Errors: pos=0.238m (min:0.084m), yaw=10.6deg
  Final State: along=0.119m, lat=0.188m, v=0.098m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=9.2
  Safety: min_clearance=0.161m
  Reward: 709.69 (mean)
  ‚úó Below threshold (need 70.0%, got 56.7%)
  üíæ Best model saved (reward: 709.69, success: 56.7%)

[Iter 105] Phase Steps: 665,600/2,000,000,000 | Reward: 887.23 | Length: 91.6 | Best Success: 56.7%
[Iter 110] Phase Steps: 697,600/2,000,000,000 | Reward: 851.10 | Length: 98.1 | Best Success: 56.7%

[EVAL] Iter 110 | Phase Steps: 697,600
  Success Rate: 66.7% (Threshold: 70.0%)
  Terminations: 20 success, 4 collision, 6 timeout
  Errors: pos=0.233m (min:0.051m), yaw=11.7deg
  Final State: along=0.131m, lat=0.176m, v=0.111m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=8.0
  Safety: min_clearance=0.152m
  Reward: 739.47 (mean)
  ‚úó Below threshold (need 70.0%, got 66.7%)
  üíæ Best model saved (reward: 739.47, success: 66.7%)

[Iter 115] Phase Steps: 729,600/2,000,000,000 | Reward: 907.34 | Length: 67.7 | Best Success: 66.7%
[Iter 120] Phase Steps: 761,600/2,000,000,000 | Reward: 1012.02 | Length: 71.3 | Best Success: 66.7%

[EVAL] Iter 120 | Phase Steps: 761,600
  Success Rate: 76.7% (Threshold: 70.0%)
  Terminations: 23 success, 0 collision, 7 timeout
  Errors: pos=0.211m (min:0.063m), yaw=9.3deg
  Final State: along=0.098m, lat=0.169m, v=0.079m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=7.4
  Safety: min_clearance=0.194m
  Reward: 1047.00 (mean)
  ‚úì PASSED threshold! (1/5 consecutive passes needed)
  üíæ Best model saved (reward: 1047.00, success: 76.7%)

[Iter 125] Phase Steps: 793,600/2,000,000,000 | Reward: 1038.46 | Length: 67.3 | Best Success: 76.7%
[Iter 130] Phase Steps: 825,600/2,000,000,000 | Reward: 1022.09 | Length: 75.7 | Best Success: 76.7%

[EVAL] Iter 130 | Phase Steps: 825,600
  Success Rate: 80.0% (Threshold: 70.0%)
  Terminations: 24 success, 1 collision, 5 timeout
  Errors: pos=0.208m (min:0.088m), yaw=7.4deg
  Final State: along=0.090m, lat=0.171m, v=0.089m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=8.7
  Safety: min_clearance=0.214m
  Reward: 1013.12 (mean)
  ‚úì PASSED threshold! (2/5 consecutive passes needed)
  üíæ Best model saved (reward: 1013.12, success: 80.0%)

[Iter 135] Phase Steps: 857,600/2,000,000,000 | Reward: 945.67 | Length: 84.4 | Best Success: 80.0%
[Iter 140] Phase Steps: 889,600/2,000,000,000 | Reward: 1039.01 | Length: 74.7 | Best Success: 80.0%

[EVAL] Iter 140 | Phase Steps: 889,600
  Success Rate: 80.0% (Threshold: 70.0%)
  Terminations: 24 success, 0 collision, 6 timeout
  Errors: pos=0.187m (min:0.095m), yaw=6.9deg
  Final State: along=0.070m, lat=0.170m, v=0.074m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=10.3
  Safety: min_clearance=0.214m
  Reward: 1033.74 (mean)
  ‚úì PASSED threshold! (3/5 consecutive passes needed)

[Iter 145] Phase Steps: 921,600/2,000,000,000 | Reward: 1010.33 | Length: 76.0 | Best Success: 80.0%
[Iter 150] Phase Steps: 953,600/2,000,000,000 | Reward: 996.82 | Length: 72.2 | Best Success: 80.0%

[EVAL] Iter 150 | Phase Steps: 953,600
  Success Rate: 83.3% (Threshold: 70.0%)
  Terminations: 25 success, 0 collision, 5 timeout
  Errors: pos=0.175m (min:0.049m), yaw=6.8deg
  Final State: along=0.068m, lat=0.158m, v=0.085m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=8.1
  Safety: min_clearance=0.196m
  Reward: 875.40 (mean)
  ‚úì PASSED threshold! (4/5 consecutive passes needed)
  üíæ Best model saved (reward: 875.40, success: 83.3%)

[Iter 155] Phase Steps: 985,600/2,000,000,000 | Reward: 1024.64 | Length: 69.5 | Best Success: 83.3%
[Iter 160] Phase Steps: 1,017,600/2,000,000,000 | Reward: 1068.65 | Length: 65.6 | Best Success: 83.3%

[EVAL] Iter 160 | Phase Steps: 1,017,600
  Success Rate: 80.0% (Threshold: 70.0%)
  Terminations: 24 success, 2 collision, 4 timeout
  Errors: pos=0.184m (min:0.071m), yaw=8.1deg
  Final State: along=0.086m, lat=0.157m, v=0.105m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=8.6
  Safety: min_clearance=0.180m
  Reward: 714.96 (mean)
  ‚úì PASSED threshold! (5/5 consecutive passes needed)

================================================================================
‚úì‚úì‚úì PHASE 4 COMPLETE - SUCCESS THRESHOLD ACHIEVED!
================================================================================
Phase timesteps: 1,017,600
Success rate: 80.0% (threshold: 70.0%)
Best success rate: 83.3%
Best eval reward: 1047.00
================================================================================


================================================================================
PHASE 4 SUMMARY
================================================================================
  Phase timesteps: 1,017,600
  Best eval reward: 1047.00
  Best success rate: 83.3%
  Status: ‚úì SUCCESS (threshold 70.0% achieved)
================================================================================

================================================================================
PHASE 5/7: Phase 4: Full Bay Randomization
================================================================================
Phase: Phase 4: Full Bay Randomization
Description: Bay can be anywhere (X and Y). Spawn varies widely. Must generalize to any bay position.
Target Timesteps: 550,000,000
Success Threshold: 70.0%

Training Config:
  - Learning Rate: 3e-4
  - Entropy Coeff: 0.015
  - Batch Size: 6000
  - SGD Iterations: 12
  - Eval Interval: 10

Loading policy weights from: checkpoints/curriculum/curriculum_20260129_205818/phase4a_random_bay_y_small/final_checkpoint
‚úì Loaded 13 weight tensors from checkpoint
Policy weights loaded successfully (warm start)

================================================================================
TRAINING PHASE 5: Phase 4: Full Bay Randomization
================================================================================
Success threshold: 70.0% (must pass 5 consecutive evaluations)
Target timesteps: ~550,000,000
Max timesteps: 5,500,000,000
Training config: lr=0.0003, entropy=0.015
Starting from global timestep: 6,400
================================================================================

[Iter 5] Phase Steps: 25,600/5,500,000,000 | Reward: -610.17 | Length: 163.4 | Best Success: 0.0%
[Iter 10] Phase Steps: 57,600/5,500,000,000 | Reward: -468.71 | Length: 181.1 | Best Success: 0.0%

[EVAL] Iter 10 | Phase Steps: 57,600
  Success Rate: 6.7% (Threshold: 70.0%)
  Terminations: 2 success, 6 collision, 22 timeout
  Errors: pos=0.786m (min:0.089m), yaw=29.6deg
  Final State: along=0.536m, lat=0.536m, v=0.096m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=4.8
  Safety: min_clearance=0.154m
  Reward: -579.81 (mean)
  ‚úó Below threshold (need 70.0%, got 6.7%)
  üíæ Best model saved (reward: -579.81, success: 6.7%)

[Iter 15] Phase Steps: 89,600/5,500,000,000 | Reward: -532.00 | Length: 194.0 | Best Success: 6.7%
[Iter 20] Phase Steps: 121,600/5,500,000,000 | Reward: -202.71 | Length: 196.1 | Best Success: 6.7%

[EVAL] Iter 20 | Phase Steps: 121,600
  Success Rate: 20.0% (Threshold: 70.0%)
  Terminations: 6 success, 4 collision, 20 timeout
  Errors: pos=0.722m (min:0.057m), yaw=29.4deg
  Final State: along=0.467m, lat=0.525m, v=0.077m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=3.0
  Safety: min_clearance=0.167m
  Reward: 81.25 (mean)
  ‚úó Below threshold (need 70.0%, got 20.0%)
  üíæ Best model saved (reward: 81.25, success: 20.0%)

[Iter 25] Phase Steps: 153,600/5,500,000,000 | Reward: 7.63 | Length: 183.8 | Best Success: 20.0%
[Iter 30] Phase Steps: 185,600/5,500,000,000 | Reward: 86.27 | Length: 146.5 | Best Success: 20.0%

[EVAL] Iter 30 | Phase Steps: 185,600
  Success Rate: 56.7% (Threshold: 70.0%)
  Terminations: 17 success, 1 collision, 12 timeout
  Errors: pos=0.417m (min:0.032m), yaw=17.8deg
  Final State: along=0.241m, lat=0.326m, v=0.066m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=3.4
  Safety: min_clearance=0.157m
  Reward: 556.55 (mean)
  ‚úó Below threshold (need 70.0%, got 56.7%)
  üíæ Best model saved (reward: 556.55, success: 56.7%)

[Iter 35] Phase Steps: 217,600/5,500,000,000 | Reward: 88.91 | Length: 168.4 | Best Success: 56.7%
[Iter 40] Phase Steps: 249,600/5,500,000,000 | Reward: 260.58 | Length: 133.3 | Best Success: 56.7%

[EVAL] Iter 40 | Phase Steps: 249,600
  Success Rate: 70.0% (Threshold: 70.0%)
  Terminations: 21 success, 0 collision, 9 timeout
  Errors: pos=0.321m (min:0.044m), yaw=10.6deg
  Final State: along=0.140m, lat=0.280m, v=0.075m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=5.8
  Safety: min_clearance=0.167m
  Reward: 864.83 (mean)
  ‚úì PASSED threshold! (1/5 consecutive passes needed)
  üíæ Best model saved (reward: 864.83, success: 70.0%)

[Iter 45] Phase Steps: 281,600/5,500,000,000 | Reward: 494.04 | Length: 131.9 | Best Success: 70.0%
[Iter 50] Phase Steps: 313,600/5,500,000,000 | Reward: 641.43 | Length: 109.9 | Best Success: 70.0%

[EVAL] Iter 50 | Phase Steps: 313,600
  Success Rate: 73.3% (Threshold: 70.0%)
  Terminations: 22 success, 1 collision, 7 timeout
  Errors: pos=0.297m (min:0.027m), yaw=14.5deg
  Final State: along=0.131m, lat=0.252m, v=0.097m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=6.9
  Safety: min_clearance=0.167m
  Reward: 1166.13 (mean)
  ‚úì PASSED threshold! (2/5 consecutive passes needed)
  üíæ Best model saved (reward: 1166.13, success: 73.3%)

[Iter 55] Phase Steps: 345,600/5,500,000,000 | Reward: 609.50 | Length: 103.1 | Best Success: 73.3%
[Iter 60] Phase Steps: 377,600/5,500,000,000 | Reward: 634.72 | Length: 89.0 | Best Success: 73.3%

[EVAL] Iter 60 | Phase Steps: 377,600
  Success Rate: 76.7% (Threshold: 70.0%)
  Terminations: 23 success, 1 collision, 6 timeout
  Errors: pos=0.256m (min:0.046m), yaw=13.5deg
  Final State: along=0.104m, lat=0.218m, v=0.093m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=6.8
  Safety: min_clearance=0.171m
  Reward: 856.41 (mean)
  ‚úì PASSED threshold! (3/5 consecutive passes needed)
  üíæ Best model saved (reward: 856.41, success: 76.7%)

[Iter 65] Phase Steps: 409,600/5,500,000,000 | Reward: 655.57 | Length: 100.6 | Best Success: 76.7%
[Iter 70] Phase Steps: 441,600/5,500,000,000 | Reward: 709.59 | Length: 87.2 | Best Success: 76.7%

[EVAL] Iter 70 | Phase Steps: 441,600
  Success Rate: 63.3% (Threshold: 70.0%)
  Terminations: 19 success, 4 collision, 7 timeout
  Errors: pos=0.297m (min:0.028m), yaw=19.7deg
  Final State: along=0.149m, lat=0.237m, v=0.116m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=6.5
  Safety: min_clearance=0.136m
  Reward: 802.33 (mean)
  ‚úó Below threshold (need 70.0%, got 63.3%)

[Iter 75] Phase Steps: 473,600/5,500,000,000 | Reward: 866.61 | Length: 81.2 | Best Success: 76.7%
[Iter 80] Phase Steps: 505,600/5,500,000,000 | Reward: 799.83 | Length: 68.9 | Best Success: 76.7%

[EVAL] Iter 80 | Phase Steps: 505,600
  Success Rate: 73.3% (Threshold: 70.0%)
  Terminations: 22 success, 2 collision, 6 timeout
  Errors: pos=0.315m (min:0.034m), yaw=10.4deg
  Final State: along=0.189m, lat=0.243m, v=0.096m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=7.5
  Safety: min_clearance=0.134m
  Reward: 485.50 (mean)
  ‚úì PASSED threshold! (1/5 consecutive passes needed)

[Iter 85] Phase Steps: 537,600/5,500,000,000 | Reward: 955.07 | Length: 67.4 | Best Success: 76.7%
[Iter 90] Phase Steps: 569,600/5,500,000,000 | Reward: 820.88 | Length: 70.2 | Best Success: 76.7%

[EVAL] Iter 90 | Phase Steps: 569,600
  Success Rate: 73.3% (Threshold: 70.0%)
  Terminations: 22 success, 1 collision, 7 timeout
  Errors: pos=0.338m (min:0.016m), yaw=11.5deg
  Final State: along=0.199m, lat=0.261m, v=0.092m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=7.1
  Safety: min_clearance=0.132m
  Reward: 639.32 (mean)
  ‚úì PASSED threshold! (2/5 consecutive passes needed)

[Iter 95] Phase Steps: 601,600/5,500,000,000 | Reward: 956.62 | Length: 59.6 | Best Success: 76.7%
[Iter 100] Phase Steps: 633,600/5,500,000,000 | Reward: 981.29 | Length: 58.7 | Best Success: 76.7%

[EVAL] Iter 100 | Phase Steps: 633,600
  Success Rate: 83.3% (Threshold: 70.0%)
  Terminations: 25 success, 2 collision, 3 timeout
  Errors: pos=0.233m (min:0.023m), yaw=7.4deg
  Final State: along=0.169m, lat=0.151m, v=0.108m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=7.2
  Safety: min_clearance=0.155m
  Reward: 588.99 (mean)
  ‚úì PASSED threshold! (3/5 consecutive passes needed)
  üíæ Best model saved (reward: 588.99, success: 83.3%)

[Iter 105] Phase Steps: 665,600/5,500,000,000 | Reward: 1060.47 | Length: 55.5 | Best Success: 83.3%
[Iter 110] Phase Steps: 697,600/5,500,000,000 | Reward: 948.48 | Length: 61.8 | Best Success: 83.3%

[EVAL] Iter 110 | Phase Steps: 697,600
  Success Rate: 80.0% (Threshold: 70.0%)
  Terminations: 24 success, 0 collision, 6 timeout
  Errors: pos=0.273m (min:0.008m), yaw=9.5deg
  Final State: along=0.173m, lat=0.200m, v=0.087m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=7.0
  Safety: min_clearance=0.158m
  Reward: 839.13 (mean)
  ‚úì PASSED threshold! (4/5 consecutive passes needed)

[Iter 115] Phase Steps: 729,600/5,500,000,000 | Reward: 892.07 | Length: 72.6 | Best Success: 83.3%
[Iter 120] Phase Steps: 761,600/5,500,000,000 | Reward: 991.09 | Length: 62.4 | Best Success: 83.3%

[EVAL] Iter 120 | Phase Steps: 761,600
  Success Rate: 86.7% (Threshold: 70.0%)
  Terminations: 26 success, 1 collision, 3 timeout
  Errors: pos=0.205m (min:0.011m), yaw=8.0deg
  Final State: along=0.148m, lat=0.137m, v=0.091m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=8.7
  Safety: min_clearance=0.140m
  Reward: 1191.38 (mean)
  ‚úì PASSED threshold! (5/5 consecutive passes needed)
  üíæ Best model saved (reward: 1191.38, success: 86.7%)

================================================================================
‚úì‚úì‚úì PHASE 5 COMPLETE - SUCCESS THRESHOLD ACHIEVED!
================================================================================
Phase timesteps: 761,600
Success rate: 86.7% (threshold: 70.0%)
Best success rate: 86.7%
Best eval reward: 1191.38
================================================================================


================================================================================
PHASE 5 SUMMARY
================================================================================
  Phase timesteps: 761,600
  Best eval reward: 1191.38
  Best success rate: 86.7%
  Status: ‚úì SUCCESS (threshold 70.0% achieved)
================================================================================

================================================================================
PHASE 6/7: Phase 5: Neighbor Position Jitter
================================================================================
Phase: Phase 5: Neighbor Position Jitter
Description: Neighbors have random ¬±5cm jitter. Must handle tight/wide gaps.
Target Timesteps: 60,000,000
Success Threshold: 65.0%

Training Config:
  - Learning Rate: 2e-4
  - Entropy Coeff: 0.01
  - Batch Size: 6000
  - SGD Iterations: 12
  - Eval Interval: 10

Loading policy weights from: checkpoints/curriculum/curriculum_20260129_205818/phase4_random_bay_full/final_checkpoint
‚úì Loaded 13 weight tensors from checkpoint
Policy weights loaded successfully (warm start)

================================================================================
TRAINING PHASE 6: Phase 5: Neighbor Position Jitter
================================================================================
Success threshold: 65.0% (must pass 5 consecutive evaluations)
Target timesteps: ~60,000,000
Max timesteps: 600,000,000
Training config: lr=0.0002, entropy=0.01
Starting from global timestep: 6,400
================================================================================

[Iter 5] Phase Steps: 25,600/600,000,000 | Reward: -571.18 | Length: 148.0 | Best Success: 0.0%
[Iter 10] Phase Steps: 57,600/600,000,000 | Reward: -255.85 | Length: 151.3 | Best Success: 0.0%

[EVAL] Iter 10 | Phase Steps: 57,600
  Success Rate: 40.0% (Threshold: 65.0%)
  Terminations: 12 success, 2 collision, 16 timeout
  Errors: pos=0.688m (min:0.145m), yaw=10.2deg
  Final State: along=0.454m, lat=0.470m, v=0.048m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=3.9
  Safety: min_clearance=0.179m
  Reward: -221.88 (mean)
  ‚úó Below threshold (need 65.0%, got 40.0%)
  üíæ Best model saved (reward: -221.88, success: 40.0%)

[Iter 15] Phase Steps: 89,600/600,000,000 | Reward: -258.79 | Length: 177.8 | Best Success: 40.0%
[Iter 20] Phase Steps: 121,600/600,000,000 | Reward: 206.63 | Length: 140.2 | Best Success: 40.0%

[EVAL] Iter 20 | Phase Steps: 121,600
  Success Rate: 60.0% (Threshold: 65.0%)
  Terminations: 18 success, 2 collision, 10 timeout
  Errors: pos=0.505m (min:0.072m), yaw=7.3deg
  Final State: along=0.358m, lat=0.318m, v=0.080m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=4.9
  Safety: min_clearance=0.189m
  Reward: 490.13 (mean)
  ‚úó Below threshold (need 65.0%, got 60.0%)
  üíæ Best model saved (reward: 490.13, success: 60.0%)

[Iter 25] Phase Steps: 153,600/600,000,000 | Reward: 339.93 | Length: 115.2 | Best Success: 60.0%
[Iter 30] Phase Steps: 185,600/600,000,000 | Reward: 441.45 | Length: 78.7 | Best Success: 60.0%

[EVAL] Iter 30 | Phase Steps: 185,600
  Success Rate: 76.7% (Threshold: 65.0%)
  Terminations: 23 success, 3 collision, 4 timeout
  Errors: pos=0.292m (min:0.061m), yaw=8.2deg
  Final State: along=0.213m, lat=0.176m, v=0.118m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=5.6
  Safety: min_clearance=0.173m
  Reward: 982.43 (mean)
  ‚úì PASSED threshold! (1/5 consecutive passes needed)
  üíæ Best model saved (reward: 982.43, success: 76.7%)

[Iter 35] Phase Steps: 217,600/600,000,000 | Reward: 666.07 | Length: 80.3 | Best Success: 76.7%
[Iter 40] Phase Steps: 249,600/600,000,000 | Reward: 741.62 | Length: 70.5 | Best Success: 76.7%

[EVAL] Iter 40 | Phase Steps: 249,600
  Success Rate: 80.0% (Threshold: 65.0%)
  Terminations: 24 success, 3 collision, 3 timeout
  Errors: pos=0.196m (min:0.040m), yaw=8.8deg
  Final State: along=0.142m, lat=0.130m, v=0.119m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=6.2
  Safety: min_clearance=0.172m
  Reward: 1176.98 (mean)
  ‚úì PASSED threshold! (2/5 consecutive passes needed)
  üíæ Best model saved (reward: 1176.98, success: 80.0%)

[Iter 45] Phase Steps: 281,600/600,000,000 | Reward: 571.43 | Length: 87.9 | Best Success: 80.0%
[Iter 50] Phase Steps: 313,600/600,000,000 | Reward: 674.51 | Length: 68.4 | Best Success: 80.0%

[EVAL] Iter 50 | Phase Steps: 313,600
  Success Rate: 80.0% (Threshold: 65.0%)
  Terminations: 24 success, 3 collision, 3 timeout
  Errors: pos=0.216m (min:0.106m), yaw=7.0deg
  Final State: along=0.136m, lat=0.157m, v=0.112m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=5.0
  Safety: min_clearance=0.169m
  Reward: 993.25 (mean)
  ‚úì PASSED threshold! (3/5 consecutive passes needed)

[Iter 55] Phase Steps: 345,600/600,000,000 | Reward: 778.60 | Length: 56.3 | Best Success: 80.0%
[Iter 60] Phase Steps: 377,600/600,000,000 | Reward: 731.83 | Length: 70.4 | Best Success: 80.0%

[EVAL] Iter 60 | Phase Steps: 377,600
  Success Rate: 86.7% (Threshold: 65.0%)
  Terminations: 26 success, 2 collision, 2 timeout
  Errors: pos=0.188m (min:0.050m), yaw=6.7deg
  Final State: along=0.111m, lat=0.142m, v=0.104m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=5.6
  Safety: min_clearance=0.176m
  Reward: 977.08 (mean)
  ‚úì PASSED threshold! (4/5 consecutive passes needed)
  üíæ Best model saved (reward: 977.08, success: 86.7%)

[Iter 65] Phase Steps: 409,600/600,000,000 | Reward: 744.54 | Length: 61.7 | Best Success: 86.7%
[Iter 70] Phase Steps: 441,600/600,000,000 | Reward: 702.81 | Length: 72.0 | Best Success: 86.7%

[EVAL] Iter 70 | Phase Steps: 441,600
  Success Rate: 90.0% (Threshold: 65.0%)
  Terminations: 27 success, 2 collision, 1 timeout
  Errors: pos=0.190m (min:0.115m), yaw=6.1deg
  Final State: along=0.116m, lat=0.143m, v=0.109m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=5.5
  Safety: min_clearance=0.220m
  Reward: 789.62 (mean)
  ‚úì PASSED threshold! (5/5 consecutive passes needed)
  üíæ Best model saved (reward: 789.62, success: 90.0%)

================================================================================
‚úì‚úì‚úì PHASE 6 COMPLETE - SUCCESS THRESHOLD ACHIEVED!
================================================================================
Phase timesteps: 441,600
Success rate: 90.0% (threshold: 65.0%)
Best success rate: 90.0%
Best eval reward: 1176.98
================================================================================


================================================================================
PHASE 6 SUMMARY
================================================================================
  Phase timesteps: 441,600
  Best eval reward: 1176.98
  Best success rate: 90.0%
  Status: ‚úì SUCCESS (threshold 65.0% achieved)
================================================================================

================================================================================
PHASE 7/7: Phase 6: Random Obstacles in Environment
================================================================================
Phase: Phase 6: Random Obstacles in Environment
Description: Full randomization + random obstacles. Maximum difficulty.
Target Timesteps: 650,000,000
Success Threshold: 60.0%

Training Config:
  - Learning Rate: 2e-4
  - Entropy Coeff: 0.02
  - Batch Size: 8000
  - SGD Iterations: 15
  - Eval Interval: 10

Loading policy weights from: checkpoints/curriculum/curriculum_20260129_205818/phase5_neighbor_jitter/final_checkpoint
‚úì Loaded 13 weight tensors from checkpoint
Policy weights loaded successfully (warm start)

================================================================================
TRAINING PHASE 7: Phase 6: Random Obstacles in Environment
================================================================================
Success threshold: 60.0% (must pass 5 consecutive evaluations)
Target timesteps: ~650,000,000
Max timesteps: 6,500,000,000
Training config: lr=0.0002, entropy=0.02
Starting from global timestep: 8,000
================================================================================

[Iter 5] Phase Steps: 32,000/6,500,000,000 | Reward: -512.32 | Length: 169.8 | Best Success: 0.0%
[Iter 10] Phase Steps: 72,000/6,500,000,000 | Reward: -132.04 | Length: 156.4 | Best Success: 0.0%

[EVAL] Iter 10 | Phase Steps: 72,000
  Success Rate: 43.3% (Threshold: 60.0%)
  Terminations: 13 success, 4 collision, 13 timeout
  Errors: pos=0.516m (min:0.027m), yaw=17.1deg
  Final State: along=0.342m, lat=0.336m, v=0.117m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=4.3
  Safety: min_clearance=0.106m
  Reward: -113.19 (mean)
  ‚úó Below threshold (need 60.0%, got 43.3%)
  üíæ Best model saved (reward: -113.19, success: 43.3%)

[Iter 15] Phase Steps: 112,000/6,500,000,000 | Reward: 34.68 | Length: 154.8 | Best Success: 43.3%
[Iter 20] Phase Steps: 152,000/6,500,000,000 | Reward: 76.48 | Length: 161.4 | Best Success: 43.3%

[EVAL] Iter 20 | Phase Steps: 152,000
  Success Rate: 50.0% (Threshold: 60.0%)
  Terminations: 15 success, 2 collision, 13 timeout
  Errors: pos=0.534m (min:0.055m), yaw=14.8deg
  Final State: along=0.381m, lat=0.335m, v=0.062m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=3.4
  Safety: min_clearance=0.193m
  Reward: 224.02 (mean)
  ‚úó Below threshold (need 60.0%, got 50.0%)
  üíæ Best model saved (reward: 224.02, success: 50.0%)

[Iter 25] Phase Steps: 192,000/6,500,000,000 | Reward: 255.09 | Length: 169.5 | Best Success: 50.0%
[Iter 30] Phase Steps: 232,000/6,500,000,000 | Reward: 473.60 | Length: 134.7 | Best Success: 50.0%

[EVAL] Iter 30 | Phase Steps: 232,000
  Success Rate: 53.3% (Threshold: 60.0%)
  Terminations: 16 success, 2 collision, 12 timeout
  Errors: pos=0.442m (min:0.092m), yaw=13.1deg
  Final State: along=0.301m, lat=0.296m, v=0.087m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=3.9
  Safety: min_clearance=0.182m
  Reward: 889.44 (mean)
  ‚úó Below threshold (need 60.0%, got 53.3%)
  üíæ Best model saved (reward: 889.44, success: 53.3%)

[Iter 35] Phase Steps: 272,000/6,500,000,000 | Reward: 508.55 | Length: 108.3 | Best Success: 53.3%
[Iter 40] Phase Steps: 312,000/6,500,000,000 | Reward: 872.08 | Length: 71.9 | Best Success: 53.3%

[EVAL] Iter 40 | Phase Steps: 312,000
  Success Rate: 86.7% (Threshold: 60.0%)
  Terminations: 26 success, 1 collision, 3 timeout
  Errors: pos=0.219m (min:0.036m), yaw=7.8deg
  Final State: along=0.132m, lat=0.163m, v=0.107m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=5.1
  Safety: min_clearance=0.172m
  Reward: 862.35 (mean)
  ‚úì PASSED threshold! (1/5 consecutive passes needed)
  üíæ Best model saved (reward: 862.35, success: 86.7%)

[Iter 45] Phase Steps: 352,000/6,500,000,000 | Reward: 823.50 | Length: 62.0 | Best Success: 86.7%
[Iter 50] Phase Steps: 392,000/6,500,000,000 | Reward: 829.85 | Length: 58.2 | Best Success: 86.7%

[EVAL] Iter 50 | Phase Steps: 392,000
  Success Rate: 90.0% (Threshold: 60.0%)
  Terminations: 27 success, 1 collision, 2 timeout
  Errors: pos=0.134m (min:0.068m), yaw=11.7deg
  Final State: along=0.089m, lat=0.089m, v=0.113m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=6.0
  Safety: min_clearance=0.160m
  Reward: 1196.51 (mean)
  ‚úì PASSED threshold! (2/5 consecutive passes needed)
  üíæ Best model saved (reward: 1196.51, success: 90.0%)

[Iter 55] Phase Steps: 432,000/6,500,000,000 | Reward: 900.44 | Length: 56.0 | Best Success: 90.0%
[Iter 60] Phase Steps: 472,000/6,500,000,000 | Reward: 848.09 | Length: 51.8 | Best Success: 90.0%

[EVAL] Iter 60 | Phase Steps: 472,000
  Success Rate: 86.7% (Threshold: 60.0%)
  Terminations: 26 success, 2 collision, 2 timeout
  Errors: pos=0.163m (min:0.055m), yaw=8.4deg
  Final State: along=0.115m, lat=0.107m, v=0.119m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=5.7
  Safety: min_clearance=0.167m
  Reward: 845.66 (mean)
  ‚úì PASSED threshold! (3/5 consecutive passes needed)

[Iter 65] Phase Steps: 512,000/6,500,000,000 | Reward: 917.25 | Length: 58.3 | Best Success: 90.0%
[Iter 70] Phase Steps: 552,000/6,500,000,000 | Reward: 968.50 | Length: 51.3 | Best Success: 90.0%

[EVAL] Iter 70 | Phase Steps: 552,000
  Success Rate: 90.0% (Threshold: 60.0%)
  Terminations: 27 success, 1 collision, 2 timeout
  Errors: pos=0.166m (min:0.076m), yaw=6.7deg
  Final State: along=0.120m, lat=0.111m, v=0.113m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=6.0
  Safety: min_clearance=0.182m
  Reward: 857.73 (mean)
  ‚úì PASSED threshold! (4/5 consecutive passes needed)

[Iter 75] Phase Steps: 592,000/6,500,000,000 | Reward: 964.25 | Length: 50.0 | Best Success: 90.0%
[Iter 80] Phase Steps: 632,000/6,500,000,000 | Reward: 992.12 | Length: 51.1 | Best Success: 90.0%

[EVAL] Iter 80 | Phase Steps: 632,000
  Success Rate: 86.7% (Threshold: 60.0%)
  Terminations: 26 success, 2 collision, 2 timeout
  Errors: pos=0.182m (min:0.100m), yaw=7.3deg
  Final State: along=0.112m, lat=0.140m, v=0.122m/s
  Motion: gear_switches=0.0 (max:0), steer_osc=5.9
  Safety: min_clearance=0.208m
  Reward: 939.05 (mean)
  ‚úì PASSED threshold! (5/5 consecutive passes needed)

================================================================================
‚úì‚úì‚úì PHASE 7 COMPLETE - SUCCESS THRESHOLD ACHIEVED!
================================================================================
Phase timesteps: 632,000
Success rate: 86.7% (threshold: 60.0%)
Best success rate: 90.0%
Best eval reward: 1196.51
================================================================================


================================================================================
PHASE 7 SUMMARY
================================================================================
  Phase timesteps: 632,000
  Best eval reward: 1196.51
  Best success rate: 90.0%
  Status: ‚úì SUCCESS (threshold 60.0% achieved)
================================================================================

================================================================================
CURRICULUM TRAINING COMPLETE
================================================================================

Phase Summary:
  Phase 1 (phase1_foundation): Success 100.0%, Reward 246.91
  Phase 2 (phase2_random_spawn): Success 100.0%, Reward 1191.87
  Phase 3 (phase3_random_bay_x): Success 93.3%, Reward 1187.92
  Phase 4 (phase4a_random_bay_y_small): Success 83.3%, Reward 1047.00
  Phase 5 (phase4_random_bay_full): Success 86.7%, Reward 1191.38
  Phase 6 (phase5_neighbor_jitter): Success 90.0%, Reward 1176.98
  Phase 7 (phase6_random_obstacles): Success 90.0%, Reward 1196.51

All checkpoints saved to: checkpoints/curriculum/curriculum_20260129_205818
Training log: checkpoints/curriculum/curriculum_20260129_205818/training_log.yaml

=========================================
Training complete!

Check results in: checkpoints/curriculum/

To monitor training:
  tensorboard --logdir checkpoints/curriculum/

To evaluate final model:
  python -m rl.eval_policy \
    --checkpoint checkpoints/curriculum/curriculum_*/phase6_random_obstacles/best_checkpoint \
    --num-episodes 100 --deterministic --save-trajectories
=========================================
(venv) naeem@Robotik5:~/Documents/final$ python -m rl.eval_policy \
    --checkpoint checkpoints/curriculum/curriculum_*/phase6_random_obstacles/best_checkpoint \
    --num-episodes 100 --deterministic --save-trajectories
2026-01-29 22:17:15,487	INFO worker.py:1724 -- Started a local Ray instance.
================================================================================
Loading trained policy from: checkpoints/curriculum/curriculum_20260129_205818/phase6_random_obstacles/best_checkpoint
================================================================================
/home/naeem/Documents/final/venv/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:483: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS="ignore::DeprecationWarning"
`UnifiedLogger` will be removed in Ray 2.7.
  return UnifiedLogger(config, logdir, loggers=None)
/home/naeem/Documents/final/venv/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS="ignore::DeprecationWarning"
The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.
  self._loggers.append(cls(self.config, self.logdir, self.trial))
/home/naeem/Documents/final/venv/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS="ignore::DeprecationWarning"
The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.
  self._loggers.append(cls(self.config, self.logdir, self.trial))
/home/naeem/Documents/final/venv/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS="ignore::DeprecationWarning"
The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.
  self._loggers.append(cls(self.config, self.logdir, self.trial))
(RolloutWorker pid=1408391) Exception raised in creation task: The actor died because of an error raised in its creation task, ray::RolloutWorker.__init__() (pid=1408391, ip=129.217.130.20, actor_id=d36a01f800b767f08712806b01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7c40096a5150>)
(RolloutWorker pid=1408391)   File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py", line 740, in make
(RolloutWorker pid=1408391)     env_spec = _find_spec(id)
(RolloutWorker pid=1408391)   File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py", line 537, in _find_spec
(RolloutWorker pid=1408391)     _check_version_exists(ns, name, version)
(RolloutWorker pid=1408391)   File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py", line 403, in _check_version_exists
(RolloutWorker pid=1408391)     _check_name_exists(ns, name)
(RolloutWorker pid=1408391)   File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py", line 380, in _check_name_exists
(RolloutWorker pid=1408391)     raise error.NameNotFound(
(RolloutWorker pid=1408391) gymnasium.error.NameNotFound: Environment `curriculum_parking_env` doesn't exist.
(RolloutWorker pid=1408391)
(RolloutWorker pid=1408391) During handling of the above exception, another exception occurred:
(RolloutWorker pid=1408391)
(RolloutWorker pid=1408391) ray::RolloutWorker.__init__() (pid=1408391, ip=129.217.130.20, actor_id=d36a01f800b767f08712806b01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7c40096a5150>)
(RolloutWorker pid=1408391)   File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py", line 407, in __init__
(RolloutWorker pid=1408391)     self.env = env_creator(copy.deepcopy(self.env_context))
(RolloutWorker pid=1408391)   File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/ray/rllib/env/utils.py", line 177, in _gym_env_creator
(RolloutWorker pid=1408391)     raise EnvError(ERR_MSG_INVALID_ENV_DESCRIPTOR.format(env_descriptor))
(RolloutWorker pid=1408391) ray.rllib.utils.error.EnvError: The env string you provided ('curriculum_parking_env') is:
(RolloutWorker pid=1408391) a) Not a supported/installed environment.
(RolloutWorker pid=1408391) b) Not a tune-registered environment creator.
(RolloutWorker pid=1408391) c) Not a valid env class string.
(RolloutWorker pid=1408391)
(RolloutWorker pid=1408391) Try one of the following:
(RolloutWorker pid=1408391) a) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`.
(RolloutWorker pid=1408391)    For VizDoom support: Install VizDoom
(RolloutWorker pid=1408391)    (https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md) and
(RolloutWorker pid=1408391)    `pip install vizdoomgym`.
(RolloutWorker pid=1408391)    For PyBullet support: `pip install pybullet`.
(RolloutWorker pid=1408391) b) To register your custom env, do `from ray import tune;
(RolloutWorker pid=1408391)    tune.register('[name]', lambda cfg: [return env obj from here using cfg])`.
(RolloutWorker pid=1408391)    Then in your config, do `config['env'] = [name]`.
(RolloutWorker pid=1408391) c) Make sure you provide a fully qualified classpath, e.g.:
(RolloutWorker pid=1408391)    `ray.rllib.examples.env.repeat_after_me_env.RepeatAfterMeEnv`
2026-01-29 22:17:17,278	ERROR actor_manager.py:506 -- Ray error, taking actor 1 out of service. The actor died because of an error raised in its creation task, ray::RolloutWorker.__init__() (pid=1408390, ip=129.217.130.20, actor_id=cbff2ee59706fe8e518deef701000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7a802b6550f0>)
  File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py", line 740, in make
    env_spec = _find_spec(id)
  File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py", line 537, in _find_spec
    _check_version_exists(ns, name, version)
  File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py", line 403, in _check_version_exists
    _check_name_exists(ns, name)
  File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py", line 380, in _check_name_exists
    raise error.NameNotFound(
gymnasium.error.NameNotFound: Environment `curriculum_parking_env` doesn't exist.

During handling of the above exception, another exception occurred:

ray::RolloutWorker.__init__() (pid=1408390, ip=129.217.130.20, actor_id=cbff2ee59706fe8e518deef701000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7a802b6550f0>)
  File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py", line 407, in __init__
    self.env = env_creator(copy.deepcopy(self.env_context))
  File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/ray/rllib/env/utils.py", line 177, in _gym_env_creator
    raise EnvError(ERR_MSG_INVALID_ENV_DESCRIPTOR.format(env_descriptor))
ray.rllib.utils.error.EnvError: The env string you provided ('curriculum_parking_env') is:
a) Not a supported/installed environment.
b) Not a tune-registered environment creator.
c) Not a valid env class string.

Try one of the following:
a) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`.
   For VizDoom support: Install VizDoom
   (https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md) and
   `pip install vizdoomgym`.
   For PyBullet support: `pip install pybullet`.
b) To register your custom env, do `from ray import tune;
   tune.register('[name]', lambda cfg: [return env obj from here using cfg])`.
   Then in your config, do `config['env'] = [name]`.
c) Make sure you provide a fully qualified classpath, e.g.:
   `ray.rllib.examples.env.repeat_after_me_env.RepeatAfterMeEnv`
2026-01-29 22:17:17,278	ERROR actor_manager.py:506 -- Ray error, taking actor 2 out of service. The actor died because of an error raised in its creation task, ray::RolloutWorker.__init__() (pid=1408389, ip=129.217.130.20, actor_id=31a803eab95688521a66902c01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x709f568a10f0>)
  File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py", line 740, in make
    env_spec = _find_spec(id)
  File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py", line 537, in _find_spec
    _check_version_exists(ns, name, version)
  File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py", line 403, in _check_version_exists
    _check_name_exists(ns, name)
  File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py", line 380, in _check_name_exists
    raise error.NameNotFound(
gymnasium.error.NameNotFound: Environment `curriculum_parking_env` doesn't exist.

During handling of the above exception, another exception occurred:

ray::RolloutWorker.__init__() (pid=1408389, ip=129.217.130.20, actor_id=31a803eab95688521a66902c01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x709f568a10f0>)
  File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py", line 407, in __init__
    self.env = env_creator(copy.deepcopy(self.env_context))
  File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/ray/rllib/env/utils.py", line 177, in _gym_env_creator
    raise EnvError(ERR_MSG_INVALID_ENV_DESCRIPTOR.format(env_descriptor))
ray.rllib.utils.error.EnvError: The env string you provided ('curriculum_parking_env') is:
a) Not a supported/installed environment.
b) Not a tune-registered environment creator.
c) Not a valid env class string.

Try one of the following:
a) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`.
   For VizDoom support: Install VizDoom
   (https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md) and
   `pip install vizdoomgym`.
   For PyBullet support: `pip install pybullet`.
b) To register your custom env, do `from ray import tune;
   tune.register('[name]', lambda cfg: [return env obj from here using cfg])`.
   Then in your config, do `config['env'] = [name]`.
c) Make sure you provide a fully qualified classpath, e.g.:
   `ray.rllib.examples.env.repeat_after_me_env.RepeatAfterMeEnv`
2026-01-29 22:17:17,278	ERROR actor_manager.py:506 -- Ray error, taking actor 3 out of service. The actor died because of an error raised in its creation task, ray::RolloutWorker.__init__() (pid=1408391, ip=129.217.130.20, actor_id=d36a01f800b767f08712806b01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7c40096a5150>)
  File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py", line 740, in make
    env_spec = _find_spec(id)
  File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py", line 537, in _find_spec
    _check_version_exists(ns, name, version)
  File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py", line 403, in _check_version_exists
    _check_name_exists(ns, name)
  File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py", line 380, in _check_name_exists
    raise error.NameNotFound(
gymnasium.error.NameNotFound: Environment `curriculum_parking_env` doesn't exist.

During handling of the above exception, another exception occurred:

ray::RolloutWorker.__init__() (pid=1408391, ip=129.217.130.20, actor_id=d36a01f800b767f08712806b01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7c40096a5150>)
  File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py", line 407, in __init__
    self.env = env_creator(copy.deepcopy(self.env_context))
  File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/ray/rllib/env/utils.py", line 177, in _gym_env_creator
    raise EnvError(ERR_MSG_INVALID_ENV_DESCRIPTOR.format(env_descriptor))
ray.rllib.utils.error.EnvError: The env string you provided ('curriculum_parking_env') is:
a) Not a supported/installed environment.
b) Not a tune-registered environment creator.
c) Not a valid env class string.

Try one of the following:
a) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`.
   For VizDoom support: Install VizDoom
   (https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md) and
   `pip install vizdoomgym`.
   For PyBullet support: `pip install pybullet`.
b) To register your custom env, do `from ray import tune;
   tune.register('[name]', lambda cfg: [return env obj from here using cfg])`.
   Then in your config, do `config['env'] = [name]`.
c) Make sure you provide a fully qualified classpath, e.g.:
   `ray.rllib.examples.env.repeat_after_me_env.RepeatAfterMeEnv`
2026-01-29 22:17:17,279	ERROR actor_manager.py:506 -- Ray error, taking actor 4 out of service. The actor died because of an error raised in its creation task, ray::RolloutWorker.__init__() (pid=1408392, ip=129.217.130.20, actor_id=c4c1bbddf5c871c3edb4231f01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x76236ae79120>)
  File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py", line 740, in make
    env_spec = _find_spec(id)
  File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py", line 537, in _find_spec
    _check_version_exists(ns, name, version)
  File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py", line 403, in _check_version_exists
    _check_name_exists(ns, name)
  File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py", line 380, in _check_name_exists
    raise error.NameNotFound(
gymnasium.error.NameNotFound: Environment `curriculum_parking_env` doesn't exist.

During handling of the above exception, another exception occurred:

ray::RolloutWorker.__init__() (pid=1408392, ip=129.217.130.20, actor_id=c4c1bbddf5c871c3edb4231f01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x76236ae79120>)
  File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py", line 407, in __init__
    self.env = env_creator(copy.deepcopy(self.env_context))
  File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/ray/rllib/env/utils.py", line 177, in _gym_env_creator
    raise EnvError(ERR_MSG_INVALID_ENV_DESCRIPTOR.format(env_descriptor))
ray.rllib.utils.error.EnvError: The env string you provided ('curriculum_parking_env') is:
a) Not a supported/installed environment.
b) Not a tune-registered environment creator.
c) Not a valid env class string.

Try one of the following:
a) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`.
   For VizDoom support: Install VizDoom
   (https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md) and
   `pip install vizdoomgym`.
   For PyBullet support: `pip install pybullet`.
b) To register your custom env, do `from ray import tune;
   tune.register('[name]', lambda cfg: [return env obj from here using cfg])`.
   Then in your config, do `config['env'] = [name]`.
c) Make sure you provide a fully qualified classpath, e.g.:
   `ray.rllib.examples.env.repeat_after_me_env.RepeatAfterMeEnv`
Traceback (most recent call last):
  File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py", line 159, in __init__
    self._setup(
  File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py", line 229, in _setup
    self.add_workers(
  File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py", line 616, in add_workers
    raise result.get()
  File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/ray/rllib/utils/actor_manager.py", line 487, in __fetch_result
    result = ray.get(r)
  File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 22, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2626, in get
    raise value
ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, ray::RolloutWorker.__init__() (pid=1408390, ip=129.217.130.20, actor_id=cbff2ee59706fe8e518deef701000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7a802b6550f0>)
  File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py", line 740, in make
    env_spec = _find_spec(id)
  File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py", line 537, in _find_spec
    _check_version_exists(ns, name, version)
  File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py", line 403, in _check_version_exists
    _check_name_exists(ns, name)
  File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py", line 380, in _check_name_exists
    raise error.NameNotFound(
gymnasium.error.NameNotFound: Environment `curriculum_parking_env` doesn't exist.

During handling of the above exception, another exception occurred:

ray::RolloutWorker.__init__() (pid=1408390, ip=129.217.130.20, actor_id=cbff2ee59706fe8e518deef701000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7a802b6550f0>)
  File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py", line 407, in __init__
    self.env = env_creator(copy.deepcopy(self.env_context))
  File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/ray/rllib/env/utils.py", line 177, in _gym_env_creator
    raise EnvError(ERR_MSG_INVALID_ENV_DESCRIPTOR.format(env_descriptor))
ray.rllib.utils.error.EnvError: The env string you provided ('curriculum_parking_env') is:
a) Not a supported/installed environment.
b) Not a tune-registered environment creator.
c) Not a valid env class string.

Try one of the following:
a) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`.
   For VizDoom support: Install VizDoom
   (https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md) and
   `pip install vizdoomgym`.
   For PyBullet support: `pip install pybullet`.
b) To register your custom env, do `from ray import tune;
   tune.register('[name]', lambda cfg: [return env obj from here using cfg])`.
   Then in your config, do `config['env'] = [name]`.
c) Make sure you provide a fully qualified classpath, e.g.:
   `ray.rllib.examples.env.repeat_after_me_env.RepeatAfterMeEnv`

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/usr/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/naeem/Documents/final/rl/eval_policy.py", line 229, in <module>
    evaluate_policy(args)
  File "/home/naeem/Documents/final/rl/eval_policy.py", line 56, in evaluate_policy
    algo = PPO.from_checkpoint(args.checkpoint)
  File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py", line 344, in from_checkpoint
    return Algorithm.from_state(state)
  File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py", line 372, in from_state
    new_algo = algorithm_class(config=config)
  File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py", line 516, in __init__
    super().__init__(
  File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/ray/tune/trainable/trainable.py", line 161, in __init__
    self.setup(copy.deepcopy(self.config))
  File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py", line 638, in setup
    self.workers = WorkerSet(
  File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py", line 181, in __init__
    raise e.args[0].args[2]
ray.rllib.utils.error.EnvError: The env string you provided ('curriculum_parking_env') is:
a) Not a supported/installed environment.
b) Not a tune-registered environment creator.
c) Not a valid env class string.

Try one of the following:
a) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`.
   For VizDoom support: Install VizDoom
   (https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md) and
   `pip install vizdoomgym`.
   For PyBullet support: `pip install pybullet`.
b) To register your custom env, do `from ray import tune;
   tune.register('[name]', lambda cfg: [return env obj from here using cfg])`.
   Then in your config, do `config['env'] = [name]`.
c) Make sure you provide a fully qualified classpath, e.g.:
   `ray.rllib.examples.env.repeat_after_me_env.RepeatAfterMeEnv`

(RolloutWorker pid=1408389) Exception raised in creation task: The actor died because of an error raised in its creation task, ray::RolloutWorker.__init__() (pid=1408389, ip=129.217.130.20, actor_id=31a803eab95688521a66902c01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x709f568a10f0>) [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)
(RolloutWorker pid=1408389)   File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py", line 740, in make [repeated 3x across cluster]
(RolloutWorker pid=1408389)     env_spec = _find_spec(id) [repeated 3x across cluster]
(RolloutWorker pid=1408389)   File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py", line 537, in _find_spec [repeated 3x across cluster]
(RolloutWorker pid=1408389)     _check_version_exists(ns, name, version) [repeated 3x across cluster]
(RolloutWorker pid=1408389)   File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py", line 403, in _check_version_exists [repeated 3x across cluster]
(RolloutWorker pid=1408389)     _check_name_exists(ns, name) [repeated 3x across cluster]
(RolloutWorker pid=1408389)   File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py", line 380, in _check_name_exists [repeated 3x across cluster]
(RolloutWorker pid=1408389)     raise error.NameNotFound( [repeated 3x across cluster]
(RolloutWorker pid=1408389) gymnasium.error.NameNotFound: Environment `curriculum_parking_env` doesn't exist. [repeated 3x across cluster]
(RolloutWorker pid=1408389)  [repeated 9x across cluster]
(RolloutWorker pid=1408389) During handling of the above exception, another exception occurred: [repeated 3x across cluster]
(RolloutWorker pid=1408389) ray::RolloutWorker.__init__() (pid=1408389, ip=129.217.130.20, actor_id=31a803eab95688521a66902c01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x709f568a10f0>) [repeated 3x across cluster]
(RolloutWorker pid=1408389)   File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py", line 407, in __init__ [repeated 3x across cluster]
(RolloutWorker pid=1408389)     self.env = env_creator(copy.deepcopy(self.env_context)) [repeated 3x across cluster]
(RolloutWorker pid=1408389)   File "/home/naeem/Documents/final/venv/lib/python3.10/site-packages/ray/rllib/env/utils.py", line 177, in _gym_env_creator [repeated 3x across cluster]
(RolloutWorker pid=1408389)     raise EnvError(ERR_MSG_INVALID_ENV_DESCRIPTOR.format(env_descriptor)) [repeated 3x across cluster]
(RolloutWorker pid=1408389) ray.rllib.utils.error.EnvError: The env string you provided ('curriculum_parking_env') is: [repeated 3x across cluster]
(RolloutWorker pid=1408389) a) Not a supported/installed environment. [repeated 3x across cluster]
(RolloutWorker pid=1408389) b) Not a tune-registered environment creator. [repeated 3x across cluster]
(RolloutWorker pid=1408389) c) Not a valid env class string. [repeated 3x across cluster]
(RolloutWorker pid=1408389) Try one of the following: [repeated 3x across cluster]
(RolloutWorker pid=1408389) a) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`. [repeated 3x across cluster]
(RolloutWorker pid=1408389)    For VizDoom support: Install VizDoom [repeated 3x across cluster]
(RolloutWorker pid=1408389)    (https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md) and [repeated 3x across cluster]
(RolloutWorker pid=1408389)    `pip install vizdoomgym`. [repeated 3x across cluster]
(RolloutWorker pid=1408389)    For PyBullet support: `pip install pybullet`. [repeated 3x across cluster]
(RolloutWorker pid=1408389) b) To register your custom env, do `from ray import tune; [repeated 3x across cluster]
(RolloutWorker pid=1408389)    tune.register('[name]', lambda cfg: [return env obj from here using cfg])`. [repeated 3x across cluster]
(RolloutWorker pid=1408389)    Then in your config, do `config['env'] = [name]`. [repeated 3x across cluster]
(RolloutWorker pid=1408389) c) Make sure you provide a fully qualified classpath, e.g.: [repeated 3x across cluster]
(RolloutWorker pid=1408389)    `ray.rllib.examples.env.repeat_after_me_env.RepeatAfterMeEnv` [repeated 3x across cluster]
(venv) naeem@Robotik5:~/Documents/final$
